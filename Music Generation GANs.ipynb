{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C5SW03cBx5i",
        "outputId": "8ce5fbab-befa-4a25-8c61-dc2a32553be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.25.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Collecting packaging~=23.1 (from mido>=1.1.16->pretty_midi)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592289 sha256=3ee63402ab90ba370f327b717725cc13f1cc3222bd237478c7fcde081a3547e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: packaging, mido, pretty_midi\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed mido-1.3.2 packaging-23.2 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pretty_midi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, LSTM, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pretty_midi\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the generator with additional parameters for tempo, chord progression, and instrument\n",
        "def build_generator(latent_dim, num_notes, num_instruments):\n",
        "    # Input layers for noise, tempo, chord progression, and instrument\n",
        "    noise = Input(shape=(latent_dim,))\n",
        "    tempo = Input(shape=(1,))\n",
        "    chord_progression = Input(shape=(num_chords,))\n",
        "    instrument = Input(shape=(num_instruments,))\n",
        "\n",
        "    # Concatenate the inputs\n",
        "    x = concatenate([noise, tempo, chord_progression, instrument])\n",
        "\n",
        "    # Generator network architecture\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    output_notes = Dense(num_notes, activation='sigmoid')(x)\n",
        "    return Model([noise, tempo, chord_progression, instrument], output_notes)\n",
        "\n",
        "# Define the discriminator (unchanged)\n",
        "def build_discriminator(num_notes):\n",
        "    music = Input(shape=(num_notes,))\n",
        "    x = Reshape((num_notes, 1))(music)\n",
        "    x = LSTM(512)(x)\n",
        "    validity = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(music, validity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the dimensions and parameters\n",
        "latent_dim = 100\n",
        "num_notes = 128  # Number of notes in a music piece\n",
        "num_chords = 4   # Number of chords in the chord progression\n",
        "num_instruments = 128  # Total number of instruments (adjust as needed)\n",
        "epochs = 500      # Reduced number of epochs\n",
        "batch_size = 32   # Reduced batch size for faster training # NOTE (Actually, you would increase it for faster training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parser(self, folderName):\n",
        "\n",
        "        for file in glob.glob(f\"{folderName}/*.mid\"):\n",
        "            midi = converter.parse(file)\n",
        "            print(\"Parsing %s\" % file)\n",
        "\n",
        "            notes = []\n",
        "            for element in midi.flat.elements:\n",
        "                if isinstance(element, note.Rest) and element.offset != 0:\n",
        "                    notes.append('R')\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                if isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(pitch) for pitch in element.pitches))\n",
        "\n",
        "            self.file_notes.append(notes)\n",
        "        note_set = sorted(set(note for notes in self.file_notes for note in notes))\n",
        "        self.dic_n = len(note_set)\n",
        "        # A dictionary to map notes, chords and rest to integers\n",
        "        self.transfer_dic = dict((note, number) for number, note in enumerate(note_set))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4SoIeXcJk7v"
      },
      "source": [
        "vanilla gans for piano\n",
        "train test vali included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuo42rcyCDcy",
        "outputId": "6917284d-93d9-4ccb-a0a3-4bad7c9dda70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n",
            "0 [D loss: 0.707469, acc.: 0.00%] [G loss: 0.696442]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1 [D loss: 0.699488, acc.: 4.69%] [G loss: 0.695871]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "2 [D loss: 0.697699, acc.: 0.00%] [G loss: 0.700696]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "3 [D loss: 0.696594, acc.: 50.00%] [G loss: 0.705557]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "4 [D loss: 0.696272, acc.: 50.00%] [G loss: 0.708591]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "5 [D loss: 0.695305, acc.: 50.00%] [G loss: 0.711140]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "6 [D loss: 0.695987, acc.: 50.00%] [G loss: 0.710768]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "7 [D loss: 0.697365, acc.: 50.00%] [G loss: 0.706570]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "8 [D loss: 0.699178, acc.: 50.00%] [G loss: 0.699787]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "9 [D loss: 0.698536, acc.: 50.00%] [G loss: 0.694998]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "10 [D loss: 0.697037, acc.: 0.00%] [G loss: 0.691165]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11 [D loss: 0.695003, acc.: 50.00%] [G loss: 0.687161]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "12 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.682747]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "13 [D loss: 0.691281, acc.: 50.00%] [G loss: 0.678988]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "14 [D loss: 0.689038, acc.: 50.00%] [G loss: 0.674933]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "15 [D loss: 0.688054, acc.: 50.00%] [G loss: 0.671133]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "16 [D loss: 0.685839, acc.: 50.00%] [G loss: 0.667055]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "17 [D loss: 0.684387, acc.: 50.00%] [G loss: 0.662881]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "18 [D loss: 0.680750, acc.: 50.00%] [G loss: 0.658113]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "19 [D loss: 0.679316, acc.: 50.00%] [G loss: 0.653656]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "20 [D loss: 0.675923, acc.: 50.00%] [G loss: 0.648817]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "21 [D loss: 0.676222, acc.: 50.00%] [G loss: 0.642547]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "22 [D loss: 0.673360, acc.: 50.00%] [G loss: 0.639744]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "23 [D loss: 0.668982, acc.: 50.00%] [G loss: 0.636541]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "24 [D loss: 0.666598, acc.: 50.00%] [G loss: 0.633192]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "25 [D loss: 0.664474, acc.: 50.00%] [G loss: 0.635722]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26 [D loss: 0.659778, acc.: 50.00%] [G loss: 0.648740]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "27 [D loss: 0.658516, acc.: 50.00%] [G loss: 0.657267]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "28 [D loss: 0.644433, acc.: 50.00%] [G loss: 0.660568]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "29 [D loss: 0.680473, acc.: 50.00%] [G loss: 0.624526]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "30 [D loss: 0.705247, acc.: 50.00%] [G loss: 0.566998]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "31 [D loss: 0.691153, acc.: 50.00%] [G loss: 0.584888]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "32 [D loss: 0.689058, acc.: 50.00%] [G loss: 0.598121]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "33 [D loss: 0.688385, acc.: 50.00%] [G loss: 0.604376]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "34 [D loss: 0.685170, acc.: 50.00%] [G loss: 0.609920]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "35 [D loss: 0.686879, acc.: 50.00%] [G loss: 0.609430]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "36 [D loss: 0.690605, acc.: 50.00%] [G loss: 0.613622]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "37 [D loss: 0.692603, acc.: 50.00%] [G loss: 0.608783]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "38 [D loss: 0.692095, acc.: 50.00%] [G loss: 0.616867]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "39 [D loss: 0.685290, acc.: 50.00%] [G loss: 0.624064]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "40 [D loss: 0.681809, acc.: 50.00%] [G loss: 0.630473]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "41 [D loss: 0.683294, acc.: 50.00%] [G loss: 0.635831]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "42 [D loss: 0.676875, acc.: 50.00%] [G loss: 0.640586]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "43 [D loss: 0.676891, acc.: 50.00%] [G loss: 0.645060]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "44 [D loss: 0.677009, acc.: 50.00%] [G loss: 0.649186]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "45 [D loss: 0.672503, acc.: 50.00%] [G loss: 0.653475]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "46 [D loss: 0.672951, acc.: 50.00%] [G loss: 0.658306]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "47 [D loss: 0.677127, acc.: 50.00%] [G loss: 0.663052]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "48 [D loss: 0.679756, acc.: 50.00%] [G loss: 0.657954]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "49 [D loss: 0.678487, acc.: 50.00%] [G loss: 0.657552]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "50 [D loss: 0.677930, acc.: 48.44%] [G loss: 0.665642]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "51 [D loss: 0.679474, acc.: 48.44%] [G loss: 0.668071]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "52 [D loss: 0.674805, acc.: 50.00%] [G loss: 0.676617]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "53 [D loss: 0.674984, acc.: 48.44%] [G loss: 0.684678]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "54 [D loss: 0.666770, acc.: 48.44%] [G loss: 0.693524]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "55 [D loss: 0.668461, acc.: 46.88%] [G loss: 0.700719]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "56 [D loss: 0.665850, acc.: 96.88%] [G loss: 0.710401]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "57 [D loss: 0.667000, acc.: 90.62%] [G loss: 0.722258]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "58 [D loss: 0.659047, acc.: 93.75%] [G loss: 0.734093]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "59 [D loss: 0.655728, acc.: 92.19%] [G loss: 0.748375]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "60 [D loss: 0.652340, acc.: 87.50%] [G loss: 0.764888]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "61 [D loss: 0.643972, acc.: 87.50%] [G loss: 0.784553]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "62 [D loss: 0.635768, acc.: 87.50%] [G loss: 0.812998]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "63 [D loss: 0.627363, acc.: 92.19%] [G loss: 0.873505]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "64 [D loss: 0.618223, acc.: 75.00%] [G loss: 1.039605]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "65 [D loss: 0.520677, acc.: 76.56%] [G loss: 5.553217]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "66 [D loss: 2.024484, acc.: 54.69%] [G loss: 0.832996]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "67 [D loss: 0.657875, acc.: 82.81%] [G loss: 0.742421]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "68 [D loss: 0.673060, acc.: 78.12%] [G loss: 0.727118]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "69 [D loss: 0.669304, acc.: 76.56%] [G loss: 0.730504]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "70 [D loss: 0.665425, acc.: 87.50%] [G loss: 0.732958]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "71 [D loss: 0.667091, acc.: 79.69%] [G loss: 0.732955]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "72 [D loss: 0.671157, acc.: 82.81%] [G loss: 0.731830]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "73 [D loss: 0.672979, acc.: 82.81%] [G loss: 0.731304]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "74 [D loss: 0.667714, acc.: 81.25%] [G loss: 0.734272]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "75 [D loss: 0.669192, acc.: 84.38%] [G loss: 0.733241]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "76 [D loss: 0.671291, acc.: 75.00%] [G loss: 0.725686]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "77 [D loss: 0.678772, acc.: 73.44%] [G loss: 0.728373]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "78 [D loss: 0.666066, acc.: 85.94%] [G loss: 0.732230]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "79 [D loss: 0.668950, acc.: 81.25%] [G loss: 0.733780]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "80 [D loss: 0.665155, acc.: 82.81%] [G loss: 0.737103]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "81 [D loss: 0.666292, acc.: 82.81%] [G loss: 0.740433]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "82 [D loss: 0.664982, acc.: 79.69%] [G loss: 0.743562]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "83 [D loss: 0.665851, acc.: 70.31%] [G loss: 0.746865]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "84 [D loss: 0.657826, acc.: 82.81%] [G loss: 0.749933]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "85 [D loss: 0.661985, acc.: 78.12%] [G loss: 0.752892]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "86 [D loss: 0.658578, acc.: 79.69%] [G loss: 0.756134]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "87 [D loss: 0.660222, acc.: 76.56%] [G loss: 0.759320]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "88 [D loss: 0.654949, acc.: 81.25%] [G loss: 0.762545]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "89 [D loss: 0.660346, acc.: 78.12%] [G loss: 0.765964]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "90 [D loss: 0.655434, acc.: 76.56%] [G loss: 0.769209]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "91 [D loss: 0.659491, acc.: 70.31%] [G loss: 0.772564]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "92 [D loss: 0.648005, acc.: 84.38%] [G loss: 0.775975]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "93 [D loss: 0.647214, acc.: 84.38%] [G loss: 0.779663]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "94 [D loss: 0.646563, acc.: 78.12%] [G loss: 0.783991]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "95 [D loss: 0.650084, acc.: 79.69%] [G loss: 0.788203]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "96 [D loss: 0.653630, acc.: 73.44%] [G loss: 0.792370]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "97 [D loss: 0.648844, acc.: 79.69%] [G loss: 0.796881]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "98 [D loss: 0.643274, acc.: 78.12%] [G loss: 0.801824]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "99 [D loss: 0.637382, acc.: 82.81%] [G loss: 0.807342]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "100 [D loss: 0.636782, acc.: 79.69%] [G loss: 0.813243]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "101 [D loss: 0.637493, acc.: 78.12%] [G loss: 0.819566]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "102 [D loss: 0.635730, acc.: 79.69%] [G loss: 0.826164]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "103 [D loss: 0.637670, acc.: 71.88%] [G loss: 0.833384]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "104 [D loss: 0.627827, acc.: 78.12%] [G loss: 0.842315]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "105 [D loss: 0.631053, acc.: 70.31%] [G loss: 0.852042]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "106 [D loss: 0.614000, acc.: 85.94%] [G loss: 0.865575]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "107 [D loss: 0.613859, acc.: 81.25%] [G loss: 0.881435]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "108 [D loss: 0.619671, acc.: 70.31%] [G loss: 0.899390]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "109 [D loss: 0.607449, acc.: 75.00%] [G loss: 0.923130]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "110 [D loss: 0.600529, acc.: 71.88%] [G loss: 0.950588]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "111 [D loss: 0.600612, acc.: 68.75%] [G loss: 0.982811]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "112 [D loss: 0.577817, acc.: 75.00%] [G loss: 1.022537]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "113 [D loss: 0.579501, acc.: 70.31%] [G loss: 1.063136]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "114 [D loss: 0.554098, acc.: 79.69%] [G loss: 1.105459]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "115 [D loss: 0.553801, acc.: 71.88%] [G loss: 1.145260]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "116 [D loss: 0.541193, acc.: 75.00%] [G loss: 1.267757]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "117 [D loss: 0.540130, acc.: 68.75%] [G loss: 1.575190]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "118 [D loss: 0.667068, acc.: 62.50%] [G loss: 0.987596]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "119 [D loss: 0.573519, acc.: 81.25%] [G loss: 0.971001]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "120 [D loss: 0.560525, acc.: 87.50%] [G loss: 0.989480]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "121 [D loss: 0.548202, acc.: 92.19%] [G loss: 1.017093]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "122 [D loss: 0.548582, acc.: 85.94%] [G loss: 1.045177]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "123 [D loss: 0.549782, acc.: 81.25%] [G loss: 1.058345]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "124 [D loss: 0.533031, acc.: 85.94%] [G loss: 1.070208]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "125 [D loss: 0.541209, acc.: 84.38%] [G loss: 1.071128]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "126 [D loss: 0.528534, acc.: 92.19%] [G loss: 1.072628]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "127 [D loss: 0.539493, acc.: 82.81%] [G loss: 1.077707]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "128 [D loss: 0.516813, acc.: 92.19%] [G loss: 1.083174]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "129 [D loss: 0.538993, acc.: 84.38%] [G loss: 1.072308]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "130 [D loss: 0.532848, acc.: 82.81%] [G loss: 1.075504]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "131 [D loss: 0.534698, acc.: 85.94%] [G loss: 1.070910]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "132 [D loss: 0.521613, acc.: 92.19%] [G loss: 1.128257]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "133 [D loss: 0.513550, acc.: 89.06%] [G loss: 1.249741]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "134 [D loss: 0.477493, acc.: 82.81%] [G loss: 1.653918]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "135 [D loss: 0.420749, acc.: 82.81%] [G loss: 2.181482]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "136 [D loss: 0.374933, acc.: 81.25%] [G loss: 2.998506]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "137 [D loss: 0.450555, acc.: 75.00%] [G loss: 2.258325]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "138 [D loss: 0.403141, acc.: 81.25%] [G loss: 2.232275]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "139 [D loss: 0.376109, acc.: 87.50%] [G loss: 2.615075]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "140 [D loss: 0.412163, acc.: 78.12%] [G loss: 2.445132]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "141 [D loss: 0.404881, acc.: 84.38%] [G loss: 2.024073]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "142 [D loss: 0.594670, acc.: 79.69%] [G loss: 1.072240]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "143 [D loss: 0.526213, acc.: 89.06%] [G loss: 1.048381]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "144 [D loss: 0.523737, acc.: 90.62%] [G loss: 1.065345]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "145 [D loss: 0.506419, acc.: 93.75%] [G loss: 1.099163]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "146 [D loss: 0.493847, acc.: 95.31%] [G loss: 1.152362]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "147 [D loss: 0.468606, acc.: 92.19%] [G loss: 1.288454]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "148 [D loss: 0.431489, acc.: 93.75%] [G loss: 1.752272]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "149 [D loss: 0.413032, acc.: 84.38%] [G loss: 2.232949]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "150 [D loss: 0.363410, acc.: 84.38%] [G loss: 2.795614]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "151 [D loss: 0.447911, acc.: 76.56%] [G loss: 2.152268]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "152 [D loss: 0.349306, acc.: 92.19%] [G loss: 2.180351]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "153 [D loss: 0.437198, acc.: 79.69%] [G loss: 1.612676]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "154 [D loss: 0.426844, acc.: 89.06%] [G loss: 1.456950]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "155 [D loss: 0.414683, acc.: 93.75%] [G loss: 1.449214]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "156 [D loss: 0.429912, acc.: 95.31%] [G loss: 1.414035]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "157 [D loss: 0.418248, acc.: 92.19%] [G loss: 1.443177]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "158 [D loss: 0.430067, acc.: 93.75%] [G loss: 1.346360]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "159 [D loss: 0.465670, acc.: 87.50%] [G loss: 1.421920]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "160 [D loss: 0.424696, acc.: 93.75%] [G loss: 1.767565]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "161 [D loss: 0.526751, acc.: 76.56%] [G loss: 1.205364]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "162 [D loss: 0.453353, acc.: 96.88%] [G loss: 1.164088]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "163 [D loss: 0.458001, acc.: 96.88%] [G loss: 1.188495]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "164 [D loss: 0.468547, acc.: 100.00%] [G loss: 1.071483]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "165 [D loss: 0.486839, acc.: 93.75%] [G loss: 1.092745]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "166 [D loss: 0.460689, acc.: 96.88%] [G loss: 1.151287]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "167 [D loss: 0.505406, acc.: 93.75%] [G loss: 1.047911]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "168 [D loss: 0.475734, acc.: 95.31%] [G loss: 1.296787]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "169 [D loss: 0.446733, acc.: 89.06%] [G loss: 2.167833]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "170 [D loss: 0.468550, acc.: 78.12%] [G loss: 2.184240]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "171 [D loss: 0.585568, acc.: 75.00%] [G loss: 1.183984]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "172 [D loss: 0.607885, acc.: 78.12%] [G loss: 1.993168]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "173 [D loss: 0.550294, acc.: 90.62%] [G loss: 0.968681]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "174 [D loss: 0.511548, acc.: 96.88%] [G loss: 0.949840]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "175 [D loss: 0.521053, acc.: 92.19%] [G loss: 0.985277]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "176 [D loss: 0.505569, acc.: 95.31%] [G loss: 1.040057]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "177 [D loss: 0.499919, acc.: 92.19%] [G loss: 1.114694]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "178 [D loss: 0.485149, acc.: 89.06%] [G loss: 1.217205]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "179 [D loss: 0.462574, acc.: 90.62%] [G loss: 1.398468]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "180 [D loss: 0.415534, acc.: 89.06%] [G loss: 1.955413]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "181 [D loss: 0.322638, acc.: 89.06%] [G loss: 3.703070]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "182 [D loss: 0.443747, acc.: 79.69%] [G loss: 2.376021]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "183 [D loss: 0.304346, acc.: 89.06%] [G loss: 3.831285]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "184 [D loss: 0.477244, acc.: 79.69%] [G loss: 2.021136]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "185 [D loss: 0.352338, acc.: 92.19%] [G loss: 2.799309]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "186 [D loss: 0.392273, acc.: 82.81%] [G loss: 2.830630]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "187 [D loss: 0.281339, acc.: 89.06%] [G loss: 3.495555]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "188 [D loss: 0.384185, acc.: 82.81%] [G loss: 2.631165]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "189 [D loss: 0.414940, acc.: 84.38%] [G loss: 2.418613]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "190 [D loss: 0.383567, acc.: 82.81%] [G loss: 3.001381]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "191 [D loss: 0.354635, acc.: 90.62%] [G loss: 2.355405]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "192 [D loss: 0.384485, acc.: 89.06%] [G loss: 1.312040]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "193 [D loss: 0.421100, acc.: 82.81%] [G loss: 2.498114]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "194 [D loss: 0.495815, acc.: 84.38%] [G loss: 1.402013]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "195 [D loss: 0.439942, acc.: 92.19%] [G loss: 1.315904]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "196 [D loss: 0.404618, acc.: 96.88%] [G loss: 1.379103]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "197 [D loss: 0.399369, acc.: 95.31%] [G loss: 1.420365]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "198 [D loss: 0.397059, acc.: 90.62%] [G loss: 1.466492]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "199 [D loss: 0.383070, acc.: 93.75%] [G loss: 1.567585]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "200 [D loss: 0.391422, acc.: 87.50%] [G loss: 1.652970]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "201 [D loss: 0.381378, acc.: 87.50%] [G loss: 1.922129]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "202 [D loss: 0.351471, acc.: 93.75%] [G loss: 3.028006]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "203 [D loss: 0.356409, acc.: 85.94%] [G loss: 3.004840]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "204 [D loss: 0.356914, acc.: 93.75%] [G loss: 2.733558]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "205 [D loss: 0.441184, acc.: 79.69%] [G loss: 1.517267]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "206 [D loss: 0.343720, acc.: 93.75%] [G loss: 1.696230]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "207 [D loss: 0.370055, acc.: 89.06%] [G loss: 1.823619]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "208 [D loss: 0.329533, acc.: 95.31%] [G loss: 2.477945]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "209 [D loss: 0.386592, acc.: 84.38%] [G loss: 2.276592]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "210 [D loss: 0.308793, acc.: 90.62%] [G loss: 2.658477]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "211 [D loss: 0.383815, acc.: 85.94%] [G loss: 1.664397]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "212 [D loss: 0.318076, acc.: 98.44%] [G loss: 1.751101]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "213 [D loss: 0.294666, acc.: 93.75%] [G loss: 1.901174]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "214 [D loss: 0.258731, acc.: 96.88%] [G loss: 2.351826]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "215 [D loss: 0.207440, acc.: 96.88%] [G loss: 5.122227]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "216 [D loss: 0.460236, acc.: 84.38%] [G loss: 1.118520]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "217 [D loss: 0.408664, acc.: 100.00%] [G loss: 1.313767]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "218 [D loss: 0.371077, acc.: 95.31%] [G loss: 1.775997]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "219 [D loss: 0.310428, acc.: 98.44%] [G loss: 2.210970]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "220 [D loss: 0.244681, acc.: 98.44%] [G loss: 2.908111]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "221 [D loss: 0.240531, acc.: 98.44%] [G loss: 3.521423]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "222 [D loss: 0.268828, acc.: 93.75%] [G loss: 3.597878]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "223 [D loss: 0.208951, acc.: 100.00%] [G loss: 3.895721]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "224 [D loss: 0.199630, acc.: 98.44%] [G loss: 3.988179]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "225 [D loss: 0.203697, acc.: 98.44%] [G loss: 3.617672]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "226 [D loss: 0.143985, acc.: 100.00%] [G loss: 4.548934]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "227 [D loss: 0.120067, acc.: 100.00%] [G loss: 7.281152]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "228 [D loss: 0.146685, acc.: 95.31%] [G loss: 4.102818]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "229 [D loss: 0.115299, acc.: 98.44%] [G loss: 4.293041]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "230 [D loss: 0.065368, acc.: 100.00%] [G loss: 7.700846]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "231 [D loss: 0.031664, acc.: 100.00%] [G loss: 3.735840]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "232 [D loss: 1.761750, acc.: 50.00%] [G loss: 1.529510]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "233 [D loss: 0.414253, acc.: 93.75%] [G loss: 1.355233]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "234 [D loss: 0.634242, acc.: 46.88%] [G loss: 1.002725]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "235 [D loss: 0.306195, acc.: 95.31%] [G loss: 3.108693]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "236 [D loss: 0.810033, acc.: 54.69%] [G loss: 1.692604]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "237 [D loss: 0.398908, acc.: 84.38%] [G loss: 1.717137]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "238 [D loss: 0.390301, acc.: 84.38%] [G loss: 1.776807]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "239 [D loss: 0.445837, acc.: 79.69%] [G loss: 1.816427]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "240 [D loss: 0.402016, acc.: 84.38%] [G loss: 1.857449]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "241 [D loss: 0.432770, acc.: 79.69%] [G loss: 1.870317]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "242 [D loss: 0.396580, acc.: 82.81%] [G loss: 1.893783]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "243 [D loss: 0.370157, acc.: 84.38%] [G loss: 1.929911]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "244 [D loss: 0.418298, acc.: 82.81%] [G loss: 1.934845]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "245 [D loss: 0.365635, acc.: 87.50%] [G loss: 1.956759]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "246 [D loss: 0.419079, acc.: 79.69%] [G loss: 1.964077]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "247 [D loss: 0.363053, acc.: 87.50%] [G loss: 1.992739]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "248 [D loss: 0.376617, acc.: 85.94%] [G loss: 2.022033]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "249 [D loss: 0.350942, acc.: 87.50%] [G loss: 2.055942]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "250 [D loss: 0.367867, acc.: 87.50%] [G loss: 2.072889]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "251 [D loss: 0.363177, acc.: 85.94%] [G loss: 2.101774]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "252 [D loss: 0.319951, acc.: 90.62%] [G loss: 2.141596]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "253 [D loss: 0.350245, acc.: 89.06%] [G loss: 2.150023]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "254 [D loss: 0.320264, acc.: 93.75%] [G loss: 2.181674]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "255 [D loss: 0.355154, acc.: 89.06%] [G loss: 2.235448]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "256 [D loss: 0.284389, acc.: 93.75%] [G loss: 2.309218]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "257 [D loss: 0.353539, acc.: 85.94%] [G loss: 2.375797]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "258 [D loss: 0.325501, acc.: 89.06%] [G loss: 2.531470]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "259 [D loss: 0.350783, acc.: 79.69%] [G loss: 2.660879]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "260 [D loss: 0.276834, acc.: 92.19%] [G loss: 2.946134]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "261 [D loss: 0.284472, acc.: 92.19%] [G loss: 3.349782]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "262 [D loss: 0.274116, acc.: 93.75%] [G loss: 3.493247]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "263 [D loss: 0.272329, acc.: 92.19%] [G loss: 3.937057]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "264 [D loss: 0.322753, acc.: 92.19%] [G loss: 4.264170]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "265 [D loss: 0.282910, acc.: 92.19%] [G loss: 3.263993]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "266 [D loss: 0.359172, acc.: 87.50%] [G loss: 3.750744]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "267 [D loss: 0.328788, acc.: 84.38%] [G loss: 3.388297]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "268 [D loss: 0.285232, acc.: 93.75%] [G loss: 3.606170]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "269 [D loss: 0.271366, acc.: 89.06%] [G loss: 3.795354]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "270 [D loss: 0.209362, acc.: 96.88%] [G loss: 3.575361]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "271 [D loss: 0.838255, acc.: 35.94%] [G loss: 1.417292]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "272 [D loss: 0.557516, acc.: 95.31%] [G loss: 1.308072]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "273 [D loss: 0.659905, acc.: 32.81%] [G loss: 1.446203]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "274 [D loss: 0.778242, acc.: 32.81%] [G loss: 0.909497]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "275 [D loss: 0.685436, acc.: 34.38%] [G loss: 0.700707]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "276 [D loss: 0.577926, acc.: 92.19%] [G loss: 0.829865]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "277 [D loss: 0.547700, acc.: 90.62%] [G loss: 0.910029]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "278 [D loss: 0.496654, acc.: 92.19%] [G loss: 0.994690]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "279 [D loss: 0.524982, acc.: 85.94%] [G loss: 1.088053]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "280 [D loss: 0.460155, acc.: 89.06%] [G loss: 1.210302]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "281 [D loss: 0.456560, acc.: 89.06%] [G loss: 1.375022]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "282 [D loss: 0.395737, acc.: 93.75%] [G loss: 1.612854]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "283 [D loss: 0.366795, acc.: 89.06%] [G loss: 1.968005]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "284 [D loss: 0.332454, acc.: 87.50%] [G loss: 2.416744]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "285 [D loss: 0.275217, acc.: 95.31%] [G loss: 3.177633]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "286 [D loss: 0.344502, acc.: 79.69%] [G loss: 3.921803]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "287 [D loss: 0.247723, acc.: 89.06%] [G loss: 4.320597]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "288 [D loss: 0.234813, acc.: 90.62%] [G loss: 5.648095]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "289 [D loss: 0.382103, acc.: 87.50%] [G loss: 1.868899]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "290 [D loss: 0.464715, acc.: 93.75%] [G loss: 0.873204]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "291 [D loss: 0.517393, acc.: 95.31%] [G loss: 0.872653]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "292 [D loss: 0.520633, acc.: 92.19%] [G loss: 0.932247]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "293 [D loss: 0.483198, acc.: 92.19%] [G loss: 0.995133]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "294 [D loss: 0.471772, acc.: 95.31%] [G loss: 1.063888]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "295 [D loss: 0.456016, acc.: 95.31%] [G loss: 1.137789]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "296 [D loss: 0.441392, acc.: 95.31%] [G loss: 1.222391]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "297 [D loss: 0.440879, acc.: 90.62%] [G loss: 1.315192]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "298 [D loss: 0.380195, acc.: 96.88%] [G loss: 1.428812]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "299 [D loss: 0.373829, acc.: 95.31%] [G loss: 1.572048]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "300 [D loss: 0.347993, acc.: 95.31%] [G loss: 1.733672]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "301 [D loss: 0.311800, acc.: 96.88%] [G loss: 1.935984]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "302 [D loss: 0.327814, acc.: 93.75%] [G loss: 2.151186]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "303 [D loss: 0.301359, acc.: 93.75%] [G loss: 2.393538]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "304 [D loss: 0.278032, acc.: 93.75%] [G loss: 2.658604]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "305 [D loss: 0.288125, acc.: 92.19%] [G loss: 2.849125]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "306 [D loss: 0.256706, acc.: 93.75%] [G loss: 3.029711]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "307 [D loss: 0.253088, acc.: 93.75%] [G loss: 3.128016]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "308 [D loss: 0.228176, acc.: 96.88%] [G loss: 3.285258]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "309 [D loss: 0.263482, acc.: 95.31%] [G loss: 3.119626]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "310 [D loss: 0.225843, acc.: 96.88%] [G loss: 3.348712]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "311 [D loss: 0.226968, acc.: 95.31%] [G loss: 3.408087]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "312 [D loss: 0.232722, acc.: 95.31%] [G loss: 3.311677]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "313 [D loss: 0.326840, acc.: 95.31%] [G loss: 2.346495]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "314 [D loss: 0.377404, acc.: 93.75%] [G loss: 2.534762]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "315 [D loss: 0.309742, acc.: 84.38%] [G loss: 3.563017]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "316 [D loss: 0.269810, acc.: 87.50%] [G loss: 3.810349]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "317 [D loss: 0.469577, acc.: 87.50%] [G loss: 1.575679]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "318 [D loss: 0.704468, acc.: 43.75%] [G loss: 1.115617]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "319 [D loss: 0.618855, acc.: 40.62%] [G loss: 0.722645]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "320 [D loss: 0.529197, acc.: 96.88%] [G loss: 0.761962]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "321 [D loss: 0.534292, acc.: 93.75%] [G loss: 0.800691]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "322 [D loss: 0.494090, acc.: 98.44%] [G loss: 0.855888]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "323 [D loss: 0.494766, acc.: 92.19%] [G loss: 0.920152]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "324 [D loss: 0.491692, acc.: 90.62%] [G loss: 1.040750]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "325 [D loss: 0.459173, acc.: 93.75%] [G loss: 1.235554]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "326 [D loss: 0.402397, acc.: 93.75%] [G loss: 1.545018]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "327 [D loss: 0.357082, acc.: 90.62%] [G loss: 1.941965]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "328 [D loss: 0.411151, acc.: 87.50%] [G loss: 1.849064]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "329 [D loss: 0.353786, acc.: 92.19%] [G loss: 2.137284]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "330 [D loss: 0.386970, acc.: 82.81%] [G loss: 2.192121]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "331 [D loss: 0.421986, acc.: 84.38%] [G loss: 2.103111]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "332 [D loss: 0.346021, acc.: 89.06%] [G loss: 2.413013]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "333 [D loss: 0.374135, acc.: 89.06%] [G loss: 2.350548]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "334 [D loss: 0.398044, acc.: 85.94%] [G loss: 2.376201]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "335 [D loss: 0.335242, acc.: 84.38%] [G loss: 2.956892]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "336 [D loss: 0.328037, acc.: 85.94%] [G loss: 3.078223]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "337 [D loss: 0.250780, acc.: 93.75%] [G loss: 3.574001]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "338 [D loss: 0.182483, acc.: 93.75%] [G loss: 4.274846]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "339 [D loss: 0.295785, acc.: 87.50%] [G loss: 4.748139]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "340 [D loss: 0.137880, acc.: 96.88%] [G loss: 5.095541]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "341 [D loss: 0.243413, acc.: 90.62%] [G loss: 4.693414]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "342 [D loss: 0.140543, acc.: 96.88%] [G loss: 4.270122]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "343 [D loss: 0.184362, acc.: 93.75%] [G loss: 4.463071]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "344 [D loss: 0.210835, acc.: 92.19%] [G loss: 4.432437]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "345 [D loss: 0.167137, acc.: 96.88%] [G loss: 4.465019]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "346 [D loss: 0.223313, acc.: 92.19%] [G loss: 4.550761]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "347 [D loss: 0.117558, acc.: 96.88%] [G loss: 6.142769]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "348 [D loss: 0.815504, acc.: 46.88%] [G loss: 3.175802]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "349 [D loss: 0.758189, acc.: 76.56%] [G loss: 2.164579]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "350 [D loss: 0.616556, acc.: 76.56%] [G loss: 2.371450]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "351 [D loss: 0.405019, acc.: 81.25%] [G loss: 2.418181]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "352 [D loss: 0.376513, acc.: 84.38%] [G loss: 2.456995]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "353 [D loss: 0.438067, acc.: 71.88%] [G loss: 2.338850]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "354 [D loss: 0.378439, acc.: 85.94%] [G loss: 2.360849]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "355 [D loss: 0.407487, acc.: 73.44%] [G loss: 2.321864]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "356 [D loss: 0.327311, acc.: 90.62%] [G loss: 2.543492]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "357 [D loss: 0.364766, acc.: 85.94%] [G loss: 2.719404]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "358 [D loss: 0.391703, acc.: 81.25%] [G loss: 2.495106]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "359 [D loss: 0.397106, acc.: 82.81%] [G loss: 2.325024]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "360 [D loss: 0.371420, acc.: 87.50%] [G loss: 2.458943]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "361 [D loss: 0.372490, acc.: 82.81%] [G loss: 2.575707]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "362 [D loss: 0.326157, acc.: 85.94%] [G loss: 2.665788]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "363 [D loss: 0.388886, acc.: 87.50%] [G loss: 2.567860]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "364 [D loss: 0.346347, acc.: 85.94%] [G loss: 2.739723]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "365 [D loss: 0.325943, acc.: 85.94%] [G loss: 2.939503]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "366 [D loss: 0.350774, acc.: 85.94%] [G loss: 2.685445]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "367 [D loss: 0.304507, acc.: 92.19%] [G loss: 2.866627]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "368 [D loss: 0.382111, acc.: 85.94%] [G loss: 2.666352]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "369 [D loss: 0.288748, acc.: 92.19%] [G loss: 2.878984]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "370 [D loss: 0.352716, acc.: 84.38%] [G loss: 2.723093]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "371 [D loss: 0.294425, acc.: 92.19%] [G loss: 2.994886]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "372 [D loss: 0.349290, acc.: 82.81%] [G loss: 2.886023]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "373 [D loss: 0.269445, acc.: 90.62%] [G loss: 3.021244]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "374 [D loss: 0.360732, acc.: 87.50%] [G loss: 2.726370]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "375 [D loss: 0.298752, acc.: 90.62%] [G loss: 2.765359]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "376 [D loss: 0.328995, acc.: 89.06%] [G loss: 2.985843]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "377 [D loss: 0.294132, acc.: 85.94%] [G loss: 3.211605]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "378 [D loss: 0.282842, acc.: 92.19%] [G loss: 3.331995]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "379 [D loss: 0.259590, acc.: 93.75%] [G loss: 3.226808]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "380 [D loss: 0.292902, acc.: 90.62%] [G loss: 3.088473]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "381 [D loss: 0.357470, acc.: 87.50%] [G loss: 3.217059]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "382 [D loss: 0.268822, acc.: 89.06%] [G loss: 3.201777]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "383 [D loss: 0.267547, acc.: 93.75%] [G loss: 3.254478]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "384 [D loss: 0.239417, acc.: 98.44%] [G loss: 3.614069]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "385 [D loss: 0.311750, acc.: 89.06%] [G loss: 3.527704]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "386 [D loss: 0.314643, acc.: 85.94%] [G loss: 2.934360]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "387 [D loss: 0.321283, acc.: 85.94%] [G loss: 2.643464]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "388 [D loss: 0.244557, acc.: 96.88%] [G loss: 2.906003]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "389 [D loss: 0.206665, acc.: 96.88%] [G loss: 3.480208]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "390 [D loss: 0.259497, acc.: 93.75%] [G loss: 3.944600]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "391 [D loss: 0.253978, acc.: 87.50%] [G loss: 3.694421]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "392 [D loss: 0.264182, acc.: 95.31%] [G loss: 3.187176]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "393 [D loss: 0.249462, acc.: 93.75%] [G loss: 4.054175]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "394 [D loss: 0.206717, acc.: 93.75%] [G loss: 3.783838]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "395 [D loss: 0.253855, acc.: 90.62%] [G loss: 3.664386]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "396 [D loss: 0.382800, acc.: 84.38%] [G loss: 2.685293]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "397 [D loss: 0.221512, acc.: 96.88%] [G loss: 2.678107]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "398 [D loss: 0.223236, acc.: 96.88%] [G loss: 3.167974]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "399 [D loss: 0.226630, acc.: 93.75%] [G loss: 3.769274]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "400 [D loss: 0.190261, acc.: 95.31%] [G loss: 4.066059]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "401 [D loss: 0.195694, acc.: 98.44%] [G loss: 4.995446]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "402 [D loss: 0.253732, acc.: 89.06%] [G loss: 3.559451]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "403 [D loss: 0.281387, acc.: 96.88%] [G loss: 1.347838]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "404 [D loss: 0.344304, acc.: 93.75%] [G loss: 1.438799]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "405 [D loss: 0.359160, acc.: 92.19%] [G loss: 2.055499]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "406 [D loss: 0.203897, acc.: 93.75%] [G loss: 2.832937]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "407 [D loss: 0.193200, acc.: 96.88%] [G loss: 2.939579]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "408 [D loss: 0.203151, acc.: 96.88%] [G loss: 2.966019]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "409 [D loss: 0.160431, acc.: 98.44%] [G loss: 3.757826]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "410 [D loss: 0.180748, acc.: 95.31%] [G loss: 3.165101]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "411 [D loss: 0.189368, acc.: 95.31%] [G loss: 3.444335]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "412 [D loss: 0.145132, acc.: 93.75%] [G loss: 4.377076]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "413 [D loss: 0.155406, acc.: 96.88%] [G loss: 4.124408]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "414 [D loss: 0.086809, acc.: 100.00%] [G loss: 4.620600]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "415 [D loss: 0.141521, acc.: 96.88%] [G loss: 3.111963]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "416 [D loss: 0.122079, acc.: 100.00%] [G loss: 3.763433]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "417 [D loss: 0.101636, acc.: 98.44%] [G loss: 4.237988]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "418 [D loss: 0.200714, acc.: 95.31%] [G loss: 3.580007]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "419 [D loss: 0.079587, acc.: 100.00%] [G loss: 3.911028]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "420 [D loss: 0.080441, acc.: 100.00%] [G loss: 4.148750]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "421 [D loss: 0.077481, acc.: 98.44%] [G loss: 4.253678]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "422 [D loss: 0.087047, acc.: 98.44%] [G loss: 4.510544]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "423 [D loss: 0.043896, acc.: 100.00%] [G loss: 4.857500]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "424 [D loss: 0.080173, acc.: 98.44%] [G loss: 4.306086]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "425 [D loss: 0.100908, acc.: 96.88%] [G loss: 4.621327]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "426 [D loss: 0.082725, acc.: 100.00%] [G loss: 4.773546]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "427 [D loss: 0.102164, acc.: 98.44%] [G loss: 4.745298]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "428 [D loss: 0.086457, acc.: 98.44%] [G loss: 4.704636]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "429 [D loss: 0.049333, acc.: 100.00%] [G loss: 5.088386]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "430 [D loss: 0.073723, acc.: 98.44%] [G loss: 4.867792]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "431 [D loss: 0.150487, acc.: 98.44%] [G loss: 0.087734]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "432 [D loss: 2.545256, acc.: 34.38%] [G loss: 0.912394]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "433 [D loss: 0.411381, acc.: 100.00%] [G loss: 0.942900]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "434 [D loss: 0.438884, acc.: 100.00%] [G loss: 0.962149]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "435 [D loss: 0.434034, acc.: 98.44%] [G loss: 0.977436]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "436 [D loss: 0.444457, acc.: 98.44%] [G loss: 0.991213]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "437 [D loss: 0.425784, acc.: 98.44%] [G loss: 1.006051]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "438 [D loss: 0.423352, acc.: 100.00%] [G loss: 1.021278]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "439 [D loss: 0.435439, acc.: 96.88%] [G loss: 1.036500]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "440 [D loss: 0.420777, acc.: 96.88%] [G loss: 1.052957]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "441 [D loss: 0.433480, acc.: 95.31%] [G loss: 1.068756]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "442 [D loss: 0.394216, acc.: 100.00%] [G loss: 1.087544]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "443 [D loss: 0.407605, acc.: 96.88%] [G loss: 1.106098]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "444 [D loss: 0.423309, acc.: 98.44%] [G loss: 1.124192]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "445 [D loss: 0.411329, acc.: 96.88%] [G loss: 1.143649]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "446 [D loss: 0.401262, acc.: 95.31%] [G loss: 1.164160]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "447 [D loss: 0.398335, acc.: 96.88%] [G loss: 1.187738]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "448 [D loss: 0.385790, acc.: 96.88%] [G loss: 1.213781]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "449 [D loss: 0.388596, acc.: 98.44%] [G loss: 1.241181]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "450 [D loss: 0.351319, acc.: 98.44%] [G loss: 1.274068]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "451 [D loss: 0.394248, acc.: 92.19%] [G loss: 1.304863]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "452 [D loss: 0.355645, acc.: 100.00%] [G loss: 1.345193]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "453 [D loss: 0.365780, acc.: 93.75%] [G loss: 1.391706]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "454 [D loss: 0.348000, acc.: 95.31%] [G loss: 1.442549]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "455 [D loss: 0.341139, acc.: 96.88%] [G loss: 1.498559]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "456 [D loss: 0.340839, acc.: 98.44%] [G loss: 1.561280]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "457 [D loss: 0.308255, acc.: 96.88%] [G loss: 1.639244]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "458 [D loss: 0.300242, acc.: 96.88%] [G loss: 1.728272]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "459 [D loss: 0.278343, acc.: 98.44%] [G loss: 1.842170]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "460 [D loss: 0.310235, acc.: 92.19%] [G loss: 1.946728]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "461 [D loss: 0.250154, acc.: 98.44%] [G loss: 2.087793]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "462 [D loss: 0.237441, acc.: 96.88%] [G loss: 2.290369]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "463 [D loss: 0.250150, acc.: 95.31%] [G loss: 2.349659]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "464 [D loss: 0.269504, acc.: 93.75%] [G loss: 2.354299]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "465 [D loss: 0.210080, acc.: 95.31%] [G loss: 2.580718]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "466 [D loss: 0.221109, acc.: 93.75%] [G loss: 2.790987]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "467 [D loss: 0.227189, acc.: 96.88%] [G loss: 3.088939]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "468 [D loss: 0.271536, acc.: 95.31%] [G loss: 2.498525]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "469 [D loss: 0.192967, acc.: 100.00%] [G loss: 2.878955]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "470 [D loss: 0.187566, acc.: 98.44%] [G loss: 3.091151]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "471 [D loss: 0.180005, acc.: 95.31%] [G loss: 3.064036]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "472 [D loss: 0.180263, acc.: 96.88%] [G loss: 3.907463]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "473 [D loss: 0.175926, acc.: 96.88%] [G loss: 3.997296]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "474 [D loss: 0.131464, acc.: 98.44%] [G loss: 3.934536]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "475 [D loss: 0.156083, acc.: 98.44%] [G loss: 3.571876]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "476 [D loss: 0.227184, acc.: 98.44%] [G loss: 3.473071]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "477 [D loss: 0.221040, acc.: 93.75%] [G loss: 2.778543]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "478 [D loss: 0.207971, acc.: 96.88%] [G loss: 2.765118]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "479 [D loss: 0.210938, acc.: 95.31%] [G loss: 2.876380]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "480 [D loss: 0.223845, acc.: 98.44%] [G loss: 3.106922]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "481 [D loss: 0.201052, acc.: 95.31%] [G loss: 3.300771]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "482 [D loss: 0.149963, acc.: 98.44%] [G loss: 3.546671]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "483 [D loss: 0.157082, acc.: 98.44%] [G loss: 3.696085]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "484 [D loss: 0.185202, acc.: 96.88%] [G loss: 3.758178]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "485 [D loss: 0.136692, acc.: 98.44%] [G loss: 3.839756]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "486 [D loss: 0.121099, acc.: 98.44%] [G loss: 4.032128]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "487 [D loss: 0.127248, acc.: 100.00%] [G loss: 4.177695]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "488 [D loss: 0.135520, acc.: 98.44%] [G loss: 4.376223]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "489 [D loss: 0.110188, acc.: 100.00%] [G loss: 4.684362]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "490 [D loss: 0.089523, acc.: 98.44%] [G loss: 4.993471]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "491 [D loss: 0.085039, acc.: 98.44%] [G loss: 5.072008]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "492 [D loss: 0.083644, acc.: 98.44%] [G loss: 4.867493]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "493 [D loss: 0.060425, acc.: 100.00%] [G loss: 5.262672]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "494 [D loss: 0.050572, acc.: 100.00%] [G loss: 5.638268]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "495 [D loss: 0.073901, acc.: 96.88%] [G loss: 4.698056]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "496 [D loss: 0.042780, acc.: 100.00%] [G loss: 5.184615]\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "497 [D loss: 0.015139, acc.: 100.00%] [G loss: 6.435539]\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "498 [D loss: 0.018411, acc.: 100.00%] [G loss: 6.110250]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "499 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Accuracy on generated music samples: 1.0\n",
            "1/1 [==============================] - 0s 120ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Placeholder for training data (replace this with your actual training data)\n",
        "X = np.random.rand(1000, num_notes)\n",
        "X_train, X_test_validate = train_test_split(X, test_size=0.2, random_state=42)  # NOTE I am not sure why there is a need for a test/train set?\n",
        "X_test, X_validate = train_test_split(X_test_validate, test_size=0.5, random_state=42)\n",
        "\n",
        "# Build the generator and discriminator\n",
        "generator = build_generator(latent_dim, num_notes, num_instruments)\n",
        "discriminator = build_discriminator(num_notes)  # NOTE: Is output_notes the same value as num_notes?\n",
        "\n",
        "# Compile the discriminator\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# The generator takes noise, tempo, chord progression, and instrument as input and generates music\n",
        "noise = Input(shape=(latent_dim,))\n",
        "tempo = Input(shape=(1,))\n",
        "chord_progression = Input(shape=(num_chords,))\n",
        "instrument = Input(shape=(num_instruments,))\n",
        "generated_music = generator([noise, tempo, chord_progression, instrument])\n",
        "\n",
        "# For the combined model, only train the generator\n",
        "discriminator.trainable = False  # NOTE: Why would you only train the generator??\n",
        "# NOTE: When the generator is being trained, the discriminator is freezed and vice-versa.\n",
        "# NOTE: please go through this colab notebook \"https://github.com/ageron/handson-ml3/blob/main/17_autoencoders_gans_and_diffusion_models.ipynb\"\n",
        "\n",
        "# The discriminator takes generated music as input and determines validity\n",
        "validity = discriminator(generated_music)\n",
        "\n",
        "# The combined model (stacked generator and discriminator)\n",
        "combined = Model([noise, tempo, chord_progression, instrument], validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of music\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_music = X_train[idx]\n",
        "\n",
        "    # Generate a batch of new music\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Adjust the instrument to violin (41 is the program number for Violin)\n",
        "    instrument_data = np.ones((batch_size, num_notes)) * 41\n",
        "\n",
        "    # Adjust the chord progression to CFCG\n",
        "    chord_progression_data = np.array([[0, 5, 7, 10]] * batch_size)\n",
        "\n",
        "    # Adjust the tempo to 120 BPM\n",
        "    tempo_data = np.ones((batch_size, 1)) * 120\n",
        "\n",
        "    generated_music = generator.predict([noise, tempo_data, chord_progression_data, instrument_data])\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_music, np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_music, np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)  # NOTE: Put this value in an array.\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    g_loss = combined.train_on_batch([noise, tempo_data, chord_progression_data, instrument_data], np.ones((batch_size, 1)))  # NOTE: put this value in a different array\n",
        "\n",
        "    # Print progress\n",
        "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "# Testing the model\n",
        "# Select a random batch of music from the test set\n",
        "idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
        "real_music_test = X_test[idx]\n",
        "\n",
        "# Generate a batch of new music\n",
        "noise_test = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "instrument_data_test = np.ones((batch_size, num_notes)) * 41  # Adjust the instrument to violin\n",
        "chord_progression_data_test = np.array([[0, 5, 7, 10]] * batch_size)  # Adjust the chord progression to CFCG\n",
        "tempo_data_test = np.ones((batch_size, 1)) * 120  # Adjust the tempo to 120 BPM\n",
        "\n",
        "generated_music_test = generator.predict([noise_test, tempo_data_test, chord_progression_data_test, instrument_data_test])\n",
        "\n",
        "# NOTE: I am not sure if this is the evaluation metric for GANs. A better way would be to plot a simultaneous graph of loss values from\n",
        "# generator and discriminator\n",
        "\n",
        "# Evaluate the generated music with the discriminator\n",
        "accuracy = discriminator.evaluate(generated_music_test, np.zeros((batch_size, 1)))[1]\n",
        "print(\"Accuracy on generated music samples:\", accuracy)\n",
        "\n",
        "# Generate some music samples (adjust num_samples as needed)\n",
        "def generate_music(generator, num_samples=1):\n",
        "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "    tempo_data = np.ones((num_samples, 1)) * 120  # Tempo set to 120 BPM for all samples\n",
        "    chord_progression_data = np.array([[0, 5, 7, 10]] * num_samples)  # Chord progression set to CFCG for all samples\n",
        "    instrument_data = np.ones((num_samples, num_instruments)) * 41  # Instrument set to Violin (41) for all samples\n",
        "    generated_music = generator.predict([noise, tempo_data, chord_progression_data, instrument_data])\n",
        "    return generated_music\n",
        "\n",
        "# Example usage\n",
        "generated_samples = generate_music(generator, num_samples=5)\n",
        "\n",
        "# Save the generated samples as MIDI files\n",
        "for i, sample in enumerate(generated_samples):\n",
        "    midi_data = pretty_midi.PrettyMIDI()\n",
        "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
        "    piano = pretty_midi.Instrument(program=piano_program)\n",
        "    for time, note_prob in enumerate(sample):\n",
        "        if note_prob > 0.5:  # Play the note if the probability is above a threshold\n",
        "            pitch = np.random.randint(0, 128)  # Random pitch between 0 and 127\n",
        "            velocity = np.random.randint(0, 128)  # Random velocity between 0 and 127\n",
        "            note_start = time * 0.5  # Each time step is 0.5 seconds\n",
        "            note_end = note_start + 0.5\n",
        "            midi_note = pretty_midi.Note(\n",
        "                velocity=velocity, pitch=pitch, start=note_start, end=note_end)\n",
        "            piano.notes.append(midi_note)\n",
        "    midi_data.instruments.append(piano)\n",
        "    midi_data.write(f'generated_music_{i}.mid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2oYg1B_9GFj_",
        "outputId": "f9e3b399-ac6d-44f8-971e-c3f66df3511c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "1 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "2 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "3 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "4 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "5 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "6 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "7 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "8 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "9 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "10 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "11 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "12 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "13 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "14 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "15 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "16 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "17 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "18 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "19 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "20 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "21 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "22 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "23 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "24 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "25 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "26 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "27 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "28 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "29 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "30 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "31 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "32 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "33 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "34 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "35 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "36 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "37 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "38 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "39 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "40 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "41 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "42 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "43 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "44 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "45 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "46 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "47 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "48 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "49 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "50 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "51 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "52 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "53 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "54 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "55 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "56 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "57 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "58 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "59 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "60 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "61 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "62 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "63 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "64 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "65 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "66 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "67 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "68 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "69 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "70 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "71 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "72 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "73 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "74 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "75 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "76 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "77 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "78 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "79 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "80 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "81 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "82 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "83 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "84 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "85 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "86 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "87 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "88 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "89 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "90 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "91 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "92 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "93 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "94 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "95 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "96 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "97 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "98 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "99 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "100 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "101 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "102 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "103 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "104 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "105 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "106 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "107 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "108 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "109 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "110 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "111 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "112 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "113 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "114 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "115 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "116 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "117 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "118 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "119 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "120 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "121 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "122 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "123 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "124 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "125 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "126 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "127 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "128 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "129 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "130 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "131 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "132 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "133 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "134 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "135 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "136 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "137 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "138 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "139 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "140 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "141 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "142 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "143 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "144 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "145 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "146 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "147 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "148 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "149 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "150 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "151 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "152 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "153 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "154 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "155 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "156 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "157 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "158 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "159 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "160 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "161 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "162 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "163 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "164 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "165 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "166 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "167 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "168 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "169 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "170 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "171 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "172 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "173 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "174 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "175 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "176 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "177 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "178 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "179 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "180 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "181 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "182 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "183 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "184 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "185 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "186 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "187 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "188 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "189 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "190 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "191 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "192 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "193 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "194 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "195 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "196 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "197 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "198 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "199 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "200 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "201 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "202 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "203 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "204 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "205 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "206 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "207 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "208 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "209 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "210 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "211 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "212 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "213 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "214 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "215 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "216 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "217 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "218 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "219 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "220 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "221 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "222 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "223 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "224 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "225 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "226 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "227 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "228 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "229 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "230 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "231 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "232 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "233 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "234 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "235 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "236 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "237 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "238 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "239 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "240 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "241 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "242 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "243 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "244 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "245 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "246 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "247 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "248 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "249 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "250 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "251 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "252 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "253 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "254 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "255 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "256 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "257 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "258 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "259 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "260 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "261 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "262 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "263 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "264 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "265 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "266 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "267 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "268 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "269 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "270 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "271 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "272 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "273 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "274 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "275 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "276 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "277 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "278 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "279 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "280 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "281 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "282 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "283 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "284 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "285 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "286 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "287 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "288 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "289 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "290 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "291 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "292 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "293 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "294 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "295 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "296 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "297 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "298 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "299 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "300 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "301 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "302 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "303 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "304 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "305 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "306 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "307 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "308 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "309 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "310 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "311 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "312 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "313 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "314 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "315 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "316 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "317 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "318 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "319 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "320 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "321 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "322 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "323 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "324 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "325 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "326 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "327 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "328 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "329 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "330 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "331 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "332 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "333 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "334 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "335 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "336 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "337 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "338 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "339 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "340 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "341 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "342 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "343 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "344 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "345 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "346 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "347 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "348 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "349 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "350 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "351 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "352 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "353 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "354 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "355 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "356 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "357 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "358 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "359 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "360 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "361 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "362 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "363 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "364 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "365 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "366 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "367 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "368 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "369 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "370 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "371 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "372 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "373 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "374 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "375 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "376 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "377 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "378 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "379 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "380 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "381 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "382 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "383 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "384 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "385 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "386 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "387 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "388 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "389 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "390 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "391 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "392 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "393 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "394 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "395 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "396 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "397 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "398 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "399 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "400 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "401 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "402 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "403 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "404 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "405 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "406 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "407 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "408 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "409 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "410 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "411 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "412 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "413 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "414 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "415 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "416 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "417 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "418 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "419 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "420 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "421 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "422 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "423 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "424 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "425 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "426 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "427 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "428 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "429 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "430 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "431 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "432 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "433 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "434 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "435 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "436 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "437 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "438 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "439 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "440 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "441 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "442 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "443 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "444 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "445 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "446 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "447 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "448 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "449 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "450 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "451 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "452 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "453 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "454 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "455 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "456 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "457 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "458 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "459 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "460 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "461 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "462 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "463 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "464 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "465 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "466 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "467 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "468 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "469 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "470 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "471 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "472 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "473 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "474 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "475 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "476 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "477 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "478 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "479 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "480 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "481 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "482 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "483 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "484 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "485 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "486 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "487 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "488 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "489 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "490 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "491 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "492 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "493 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "494 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "495 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "496 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "497 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "498 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n",
            "499 [D loss: 0.082579, acc.: 100.00%] [G loss: 6.163241]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NOTE: uhh, I am not sure what you are trying to do with this snippet but this is not how you would plot losses. Instead, losses would be recorded\n",
        "# while the model is training not after it's finished.\n",
        "\n",
        "# NOTE: I have mentioned in the training code above about the creation of two arrays. Now take the values from those arrays and plot it. That would be your graph.\n",
        "# Training loop\n",
        "d_losses, g_losses = [], []\n",
        "for epoch in range(epochs):\n",
        "    # ...\n",
        "    d_losses.append(d_loss[0])\n",
        "    g_losses.append(g_loss)\n",
        "\n",
        "    # Print progress\n",
        "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "# Plot the discriminator and generator losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(d_losses, label='Discriminator loss')\n",
        "plt.plot(g_losses, label='Generator loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# NOTE: I'll be honest here. This graph is not looking good at all. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2No87smAGlJC"
      },
      "source": [
        "above was vanilla gan for piano with test train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dPMC-V6GoDX"
      },
      "source": [
        "below is same but with violin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQuSUijsGr25",
        "outputId": "2b350a48-5d5b-4227-f10f-bee25e8d5efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 140ms/step\n",
            "0 [D loss: 0.706652, acc.: 50.00%] [G loss: 0.671986]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1 [D loss: 0.702380, acc.: 50.00%] [G loss: 0.670520]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2 [D loss: 0.701248, acc.: 50.00%] [G loss: 0.677900]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3 [D loss: 0.698637, acc.: 50.00%] [G loss: 0.691136]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4 [D loss: 0.695293, acc.: 50.00%] [G loss: 0.706068]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "5 [D loss: 0.692597, acc.: 50.00%] [G loss: 0.720006]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "6 [D loss: 0.690222, acc.: 50.00%] [G loss: 0.732183]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "7 [D loss: 0.689628, acc.: 50.00%] [G loss: 0.740627]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "8 [D loss: 0.688077, acc.: 50.00%] [G loss: 0.749577]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9 [D loss: 0.687089, acc.: 50.00%] [G loss: 0.757918]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "10 [D loss: 0.685604, acc.: 50.00%] [G loss: 0.765296]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "11 [D loss: 0.691812, acc.: 50.00%] [G loss: 0.753025]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "12 [D loss: 0.691858, acc.: 50.00%] [G loss: 0.753028]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "13 [D loss: 0.691074, acc.: 50.00%] [G loss: 0.753523]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "14 [D loss: 0.693977, acc.: 50.00%] [G loss: 0.746330]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "15 [D loss: 0.696512, acc.: 50.00%] [G loss: 0.738551]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "16 [D loss: 0.692695, acc.: 50.00%] [G loss: 0.736993]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "17 [D loss: 0.693454, acc.: 50.00%] [G loss: 0.735494]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "18 [D loss: 0.695079, acc.: 50.00%] [G loss: 0.733259]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "19 [D loss: 0.692916, acc.: 50.00%] [G loss: 0.731953]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "20 [D loss: 0.693451, acc.: 50.00%] [G loss: 0.730278]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "21 [D loss: 0.692899, acc.: 50.00%] [G loss: 0.728684]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "22 [D loss: 0.692766, acc.: 50.00%] [G loss: 0.727150]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "23 [D loss: 0.691502, acc.: 51.56%] [G loss: 0.725911]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "24 [D loss: 0.690372, acc.: 51.56%] [G loss: 0.724982]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "25 [D loss: 0.691963, acc.: 53.12%] [G loss: 0.723341]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "26 [D loss: 0.689572, acc.: 51.56%] [G loss: 0.722074]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "27 [D loss: 0.687884, acc.: 57.81%] [G loss: 0.721121]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "28 [D loss: 0.685742, acc.: 62.50%] [G loss: 0.720378]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "29 [D loss: 0.686169, acc.: 71.88%] [G loss: 0.719798]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "30 [D loss: 0.683555, acc.: 78.12%] [G loss: 0.720576]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "31 [D loss: 0.681791, acc.: 78.12%] [G loss: 0.722713]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "32 [D loss: 0.676613, acc.: 84.38%] [G loss: 0.728768]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "33 [D loss: 0.675485, acc.: 93.75%] [G loss: 0.742991]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "34 [D loss: 0.656745, acc.: 90.62%] [G loss: 0.919025]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "35 [D loss: 0.688621, acc.: 32.81%] [G loss: 0.731101]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "36 [D loss: 0.680673, acc.: 45.31%] [G loss: 0.713422]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "37 [D loss: 0.675163, acc.: 85.94%] [G loss: 0.710523]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "38 [D loss: 0.675751, acc.: 85.94%] [G loss: 0.704271]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "39 [D loss: 0.697886, acc.: 35.94%] [G loss: 0.674801]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "40 [D loss: 0.713672, acc.: 39.06%] [G loss: 0.639021]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "41 [D loss: 0.732853, acc.: 40.62%] [G loss: 0.632143]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "42 [D loss: 0.721573, acc.: 31.25%] [G loss: 0.661281]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "43 [D loss: 0.709847, acc.: 18.75%] [G loss: 0.686345]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "44 [D loss: 0.698785, acc.: 20.31%] [G loss: 0.703837]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "45 [D loss: 0.692651, acc.: 65.62%] [G loss: 0.717243]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "46 [D loss: 0.693185, acc.: 56.25%] [G loss: 0.726623]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "47 [D loss: 0.689414, acc.: 60.94%] [G loss: 0.736401]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "48 [D loss: 0.691319, acc.: 53.12%] [G loss: 0.745932]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "49 [D loss: 0.687670, acc.: 51.56%] [G loss: 0.755275]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "50 [D loss: 0.685296, acc.: 51.56%] [G loss: 0.763903]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "51 [D loss: 0.685653, acc.: 50.00%] [G loss: 0.773323]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "52 [D loss: 0.681837, acc.: 50.00%] [G loss: 0.783556]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "53 [D loss: 0.679535, acc.: 50.00%] [G loss: 0.796482]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "54 [D loss: 0.678190, acc.: 50.00%] [G loss: 0.809017]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "55 [D loss: 0.674107, acc.: 50.00%] [G loss: 0.826176]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "56 [D loss: 0.668792, acc.: 50.00%] [G loss: 0.853103]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "57 [D loss: 0.655030, acc.: 51.56%] [G loss: 0.913371]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "58 [D loss: 0.617978, acc.: 50.00%] [G loss: 1.587096]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "59 [D loss: 0.831620, acc.: 53.12%] [G loss: 0.698309]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "60 [D loss: 0.704701, acc.: 9.38%] [G loss: 0.704052]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "61 [D loss: 0.696085, acc.: 64.06%] [G loss: 0.712067]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "62 [D loss: 0.694488, acc.: 62.50%] [G loss: 0.719490]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "63 [D loss: 0.688875, acc.: 65.62%] [G loss: 0.722672]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "64 [D loss: 0.696819, acc.: 51.56%] [G loss: 0.723772]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "65 [D loss: 0.690194, acc.: 56.25%] [G loss: 0.728597]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "66 [D loss: 0.697019, acc.: 53.12%] [G loss: 0.724487]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "67 [D loss: 0.693547, acc.: 53.12%] [G loss: 0.727058]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "68 [D loss: 0.693719, acc.: 53.12%] [G loss: 0.729247]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "69 [D loss: 0.693857, acc.: 51.56%] [G loss: 0.730700]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "70 [D loss: 0.697327, acc.: 53.12%] [G loss: 0.722267]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "71 [D loss: 0.697277, acc.: 51.56%] [G loss: 0.725032]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "72 [D loss: 0.697405, acc.: 50.00%] [G loss: 0.727679]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "73 [D loss: 0.697739, acc.: 51.56%] [G loss: 0.729937]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "74 [D loss: 0.694339, acc.: 50.00%] [G loss: 0.731555]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "75 [D loss: 0.696916, acc.: 50.00%] [G loss: 0.733597]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "76 [D loss: 0.693551, acc.: 51.56%] [G loss: 0.735505]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "77 [D loss: 0.693594, acc.: 50.00%] [G loss: 0.737102]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "78 [D loss: 0.693821, acc.: 50.00%] [G loss: 0.738837]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "79 [D loss: 0.692757, acc.: 50.00%] [G loss: 0.740707]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "80 [D loss: 0.694138, acc.: 50.00%] [G loss: 0.742072]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "81 [D loss: 0.690096, acc.: 53.12%] [G loss: 0.743497]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "82 [D loss: 0.691921, acc.: 50.00%] [G loss: 0.738595]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "83 [D loss: 0.693085, acc.: 50.00%] [G loss: 0.738439]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "84 [D loss: 0.692304, acc.: 51.56%] [G loss: 0.738555]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "85 [D loss: 0.689202, acc.: 54.69%] [G loss: 0.738691]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "86 [D loss: 0.688663, acc.: 53.12%] [G loss: 0.738800]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "87 [D loss: 0.689880, acc.: 51.56%] [G loss: 0.738893]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "88 [D loss: 0.693885, acc.: 50.00%] [G loss: 0.738311]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "89 [D loss: 0.689240, acc.: 51.56%] [G loss: 0.738030]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "90 [D loss: 0.691424, acc.: 51.56%] [G loss: 0.737241]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "91 [D loss: 0.690879, acc.: 50.00%] [G loss: 0.736661]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "92 [D loss: 0.689709, acc.: 51.56%] [G loss: 0.736239]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "93 [D loss: 0.690128, acc.: 50.00%] [G loss: 0.736027]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "94 [D loss: 0.690232, acc.: 51.56%] [G loss: 0.735535]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "95 [D loss: 0.689929, acc.: 50.00%] [G loss: 0.734999]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "96 [D loss: 0.691178, acc.: 50.00%] [G loss: 0.734617]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "97 [D loss: 0.689018, acc.: 50.00%] [G loss: 0.734298]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "98 [D loss: 0.686364, acc.: 50.00%] [G loss: 0.734114]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "99 [D loss: 0.688347, acc.: 51.56%] [G loss: 0.733681]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "100 [D loss: 0.684568, acc.: 51.56%] [G loss: 0.733882]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "101 [D loss: 0.688549, acc.: 53.12%] [G loss: 0.733376]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "102 [D loss: 0.686052, acc.: 53.12%] [G loss: 0.733131]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "103 [D loss: 0.685273, acc.: 56.25%] [G loss: 0.733275]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "104 [D loss: 0.685888, acc.: 56.25%] [G loss: 0.733131]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "105 [D loss: 0.686768, acc.: 53.12%] [G loss: 0.733034]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "106 [D loss: 0.687064, acc.: 56.25%] [G loss: 0.732388]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "107 [D loss: 0.687262, acc.: 54.69%] [G loss: 0.731759]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "108 [D loss: 0.684557, acc.: 53.12%] [G loss: 0.731484]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "109 [D loss: 0.682067, acc.: 59.38%] [G loss: 0.731594]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "110 [D loss: 0.684014, acc.: 59.38%] [G loss: 0.731729]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "111 [D loss: 0.684999, acc.: 60.94%] [G loss: 0.731514]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "112 [D loss: 0.684327, acc.: 60.94%] [G loss: 0.730884]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "113 [D loss: 0.681144, acc.: 64.06%] [G loss: 0.731200]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "114 [D loss: 0.685029, acc.: 54.69%] [G loss: 0.730936]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "115 [D loss: 0.683887, acc.: 57.81%] [G loss: 0.730701]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "116 [D loss: 0.683713, acc.: 60.94%] [G loss: 0.730689]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "117 [D loss: 0.684969, acc.: 59.38%] [G loss: 0.730406]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "118 [D loss: 0.684225, acc.: 62.50%] [G loss: 0.730080]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "119 [D loss: 0.684692, acc.: 59.38%] [G loss: 0.729327]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "120 [D loss: 0.680922, acc.: 68.75%] [G loss: 0.728655]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "121 [D loss: 0.683472, acc.: 64.06%] [G loss: 0.728273]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "122 [D loss: 0.681470, acc.: 70.31%] [G loss: 0.728466]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "123 [D loss: 0.679076, acc.: 75.00%] [G loss: 0.727741]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "124 [D loss: 0.681818, acc.: 67.19%] [G loss: 0.727163]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "125 [D loss: 0.681098, acc.: 67.19%] [G loss: 0.726824]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "126 [D loss: 0.682211, acc.: 68.75%] [G loss: 0.726351]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "127 [D loss: 0.681747, acc.: 76.56%] [G loss: 0.725643]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "128 [D loss: 0.678273, acc.: 81.25%] [G loss: 0.725418]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "129 [D loss: 0.680724, acc.: 75.00%] [G loss: 0.725046]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "130 [D loss: 0.681605, acc.: 76.56%] [G loss: 0.724226]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "131 [D loss: 0.681960, acc.: 79.69%] [G loss: 0.723576]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "132 [D loss: 0.680796, acc.: 82.81%] [G loss: 0.723781]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "133 [D loss: 0.679356, acc.: 79.69%] [G loss: 0.723297]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "134 [D loss: 0.681323, acc.: 76.56%] [G loss: 0.723761]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "135 [D loss: 0.678971, acc.: 87.50%] [G loss: 0.723484]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "136 [D loss: 0.680244, acc.: 85.94%] [G loss: 0.718961]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "137 [D loss: 0.681718, acc.: 84.38%] [G loss: 0.718829]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "138 [D loss: 0.680946, acc.: 89.06%] [G loss: 0.719278]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "139 [D loss: 0.682698, acc.: 85.94%] [G loss: 0.719703]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "140 [D loss: 0.676264, acc.: 90.62%] [G loss: 0.720537]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "141 [D loss: 0.680689, acc.: 85.94%] [G loss: 0.721084]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "142 [D loss: 0.676384, acc.: 92.19%] [G loss: 0.722236]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "143 [D loss: 0.677861, acc.: 87.50%] [G loss: 0.720219]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "144 [D loss: 0.683397, acc.: 90.62%] [G loss: 0.713464]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "145 [D loss: 0.681742, acc.: 84.38%] [G loss: 0.717344]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "146 [D loss: 0.687467, acc.: 78.12%] [G loss: 0.716935]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "147 [D loss: 0.678751, acc.: 81.25%] [G loss: 0.720612]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "148 [D loss: 0.679089, acc.: 78.12%] [G loss: 0.723702]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "149 [D loss: 0.679798, acc.: 84.38%] [G loss: 0.724837]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "150 [D loss: 0.685370, acc.: 81.25%] [G loss: 0.711259]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "151 [D loss: 0.684855, acc.: 75.00%] [G loss: 0.711521]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "152 [D loss: 0.685358, acc.: 79.69%] [G loss: 0.711681]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "153 [D loss: 0.682713, acc.: 76.56%] [G loss: 0.711920]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "154 [D loss: 0.677215, acc.: 92.19%] [G loss: 0.712491]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "155 [D loss: 0.677861, acc.: 92.19%] [G loss: 0.713561]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "156 [D loss: 0.679258, acc.: 90.62%] [G loss: 0.713701]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "157 [D loss: 0.674481, acc.: 92.19%] [G loss: 0.714796]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "158 [D loss: 0.676802, acc.: 92.19%] [G loss: 0.717150]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "159 [D loss: 0.680099, acc.: 85.94%] [G loss: 0.716625]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "160 [D loss: 0.674729, acc.: 90.62%] [G loss: 0.718957]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "161 [D loss: 0.670358, acc.: 92.19%] [G loss: 0.741801]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "162 [D loss: 0.667203, acc.: 93.75%] [G loss: 0.936184]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "163 [D loss: 1.401999, acc.: 37.50%] [G loss: 1.263971]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "164 [D loss: 0.498375, acc.: 92.19%] [G loss: 0.888625]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "165 [D loss: 0.455659, acc.: 100.00%] [G loss: 1.426846]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "166 [D loss: 0.443166, acc.: 96.88%] [G loss: 0.489690]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "167 [D loss: 1.016923, acc.: 3.12%] [G loss: 0.674626]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "168 [D loss: 0.703954, acc.: 25.00%] [G loss: 0.673401]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "169 [D loss: 0.700583, acc.: 34.38%] [G loss: 0.669997]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "170 [D loss: 0.699049, acc.: 37.50%] [G loss: 0.669784]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "171 [D loss: 0.699783, acc.: 34.38%] [G loss: 0.669277]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "172 [D loss: 0.699231, acc.: 35.94%] [G loss: 0.668516]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "173 [D loss: 0.700919, acc.: 34.38%] [G loss: 0.668238]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "174 [D loss: 0.697415, acc.: 43.75%] [G loss: 0.667873]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "175 [D loss: 0.696162, acc.: 48.44%] [G loss: 0.666747]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "176 [D loss: 0.697343, acc.: 40.62%] [G loss: 0.666163]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "177 [D loss: 0.696228, acc.: 48.44%] [G loss: 0.665849]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "178 [D loss: 0.696826, acc.: 43.75%] [G loss: 0.665518]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "179 [D loss: 0.694751, acc.: 45.31%] [G loss: 0.665205]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "180 [D loss: 0.694301, acc.: 46.88%] [G loss: 0.664770]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "181 [D loss: 0.691683, acc.: 48.44%] [G loss: 0.664358]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "182 [D loss: 0.693564, acc.: 46.88%] [G loss: 0.663974]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "183 [D loss: 0.692789, acc.: 48.44%] [G loss: 0.663618]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "184 [D loss: 0.692737, acc.: 50.00%] [G loss: 0.663166]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "185 [D loss: 0.690922, acc.: 50.00%] [G loss: 0.662717]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "186 [D loss: 0.690986, acc.: 48.44%] [G loss: 0.662262]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "187 [D loss: 0.688734, acc.: 48.44%] [G loss: 0.661784]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "188 [D loss: 0.688689, acc.: 48.44%] [G loss: 0.661249]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "189 [D loss: 0.691982, acc.: 46.88%] [G loss: 0.660795]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "190 [D loss: 0.688073, acc.: 50.00%] [G loss: 0.660337]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "191 [D loss: 0.686373, acc.: 50.00%] [G loss: 0.659860]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "192 [D loss: 0.686613, acc.: 50.00%] [G loss: 0.659411]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "193 [D loss: 0.684743, acc.: 50.00%] [G loss: 0.658882]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "194 [D loss: 0.683627, acc.: 50.00%] [G loss: 0.658271]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "195 [D loss: 0.685410, acc.: 50.00%] [G loss: 0.657770]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "196 [D loss: 0.686112, acc.: 50.00%] [G loss: 0.657302]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "197 [D loss: 0.684406, acc.: 50.00%] [G loss: 0.652691]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "198 [D loss: 0.686988, acc.: 50.00%] [G loss: 0.647998]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "199 [D loss: 0.687358, acc.: 50.00%] [G loss: 0.647583]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "200 [D loss: 0.686467, acc.: 50.00%] [G loss: 0.647180]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "201 [D loss: 0.684831, acc.: 50.00%] [G loss: 0.646802]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "202 [D loss: 0.685218, acc.: 50.00%] [G loss: 0.646410]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "203 [D loss: 0.688036, acc.: 50.00%] [G loss: 0.646161]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "204 [D loss: 0.683282, acc.: 50.00%] [G loss: 0.645666]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "205 [D loss: 0.682100, acc.: 50.00%] [G loss: 0.645214]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "206 [D loss: 0.681745, acc.: 50.00%] [G loss: 0.644835]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "207 [D loss: 0.681823, acc.: 50.00%] [G loss: 0.644387]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "208 [D loss: 0.686257, acc.: 50.00%] [G loss: 0.644166]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "209 [D loss: 0.680907, acc.: 50.00%] [G loss: 0.643673]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "210 [D loss: 0.681533, acc.: 50.00%] [G loss: 0.643346]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "211 [D loss: 0.679151, acc.: 50.00%] [G loss: 0.642997]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "212 [D loss: 0.681418, acc.: 50.00%] [G loss: 0.642779]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "213 [D loss: 0.679996, acc.: 50.00%] [G loss: 0.642474]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "214 [D loss: 0.682608, acc.: 50.00%] [G loss: 0.642281]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "215 [D loss: 0.680251, acc.: 50.00%] [G loss: 0.641794]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "216 [D loss: 0.678108, acc.: 50.00%] [G loss: 0.641572]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "217 [D loss: 0.678569, acc.: 50.00%] [G loss: 0.641104]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "218 [D loss: 0.679190, acc.: 50.00%] [G loss: 0.641023]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "219 [D loss: 0.683658, acc.: 50.00%] [G loss: 0.641170]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "220 [D loss: 0.677864, acc.: 50.00%] [G loss: 0.641278]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "221 [D loss: 0.677750, acc.: 50.00%] [G loss: 0.641124]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "222 [D loss: 0.676445, acc.: 50.00%] [G loss: 0.641167]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "223 [D loss: 0.683422, acc.: 50.00%] [G loss: 0.641047]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "224 [D loss: 0.675667, acc.: 50.00%] [G loss: 0.640946]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "225 [D loss: 0.676417, acc.: 50.00%] [G loss: 0.640906]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "226 [D loss: 0.676448, acc.: 50.00%] [G loss: 0.640821]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "227 [D loss: 0.671625, acc.: 50.00%] [G loss: 0.640589]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "228 [D loss: 0.672014, acc.: 50.00%] [G loss: 0.640084]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "229 [D loss: 0.675042, acc.: 50.00%] [G loss: 0.640056]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "230 [D loss: 0.670897, acc.: 50.00%] [G loss: 0.640016]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "231 [D loss: 0.672633, acc.: 50.00%] [G loss: 0.638642]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "232 [D loss: 0.692845, acc.: 50.00%] [G loss: 0.607078]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "233 [D loss: 0.693959, acc.: 50.00%] [G loss: 0.611242]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "234 [D loss: 0.689609, acc.: 50.00%] [G loss: 0.615102]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "235 [D loss: 0.685448, acc.: 50.00%] [G loss: 0.618505]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "236 [D loss: 0.688031, acc.: 50.00%] [G loss: 0.622193]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "237 [D loss: 0.688728, acc.: 50.00%] [G loss: 0.624715]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "238 [D loss: 0.684825, acc.: 50.00%] [G loss: 0.627295]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "239 [D loss: 0.686438, acc.: 50.00%] [G loss: 0.629685]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "240 [D loss: 0.686519, acc.: 50.00%] [G loss: 0.631882]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "241 [D loss: 0.680313, acc.: 50.00%] [G loss: 0.633788]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "242 [D loss: 0.685844, acc.: 50.00%] [G loss: 0.635985]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "243 [D loss: 0.690572, acc.: 48.44%] [G loss: 0.638505]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "244 [D loss: 0.682977, acc.: 50.00%] [G loss: 0.640625]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "245 [D loss: 0.682579, acc.: 48.44%] [G loss: 0.643068]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "246 [D loss: 0.680460, acc.: 50.00%] [G loss: 0.644657]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "247 [D loss: 0.680325, acc.: 50.00%] [G loss: 0.645988]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "248 [D loss: 0.681149, acc.: 48.44%] [G loss: 0.647544]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "249 [D loss: 0.680109, acc.: 48.44%] [G loss: 0.649535]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "250 [D loss: 0.684032, acc.: 46.88%] [G loss: 0.651464]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "251 [D loss: 0.680823, acc.: 48.44%] [G loss: 0.653134]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "252 [D loss: 0.678884, acc.: 50.00%] [G loss: 0.654899]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "253 [D loss: 0.675696, acc.: 50.00%] [G loss: 0.656848]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "254 [D loss: 0.671595, acc.: 50.00%] [G loss: 0.658738]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "255 [D loss: 0.683570, acc.: 43.75%] [G loss: 0.661399]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "256 [D loss: 0.675102, acc.: 50.00%] [G loss: 0.664031]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "257 [D loss: 0.672888, acc.: 48.44%] [G loss: 0.666521]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "258 [D loss: 0.676833, acc.: 46.88%] [G loss: 0.669144]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "259 [D loss: 0.672038, acc.: 50.00%] [G loss: 0.672373]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "260 [D loss: 0.673195, acc.: 43.75%] [G loss: 0.676850]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "261 [D loss: 0.665893, acc.: 48.44%] [G loss: 0.681674]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "262 [D loss: 0.668068, acc.: 48.44%] [G loss: 0.687623]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "263 [D loss: 0.668139, acc.: 46.88%] [G loss: 0.695662]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "264 [D loss: 0.657924, acc.: 96.88%] [G loss: 0.708095]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "265 [D loss: 0.642263, acc.: 100.00%] [G loss: 0.731887]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "266 [D loss: 0.632863, acc.: 98.44%] [G loss: 0.803313]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "267 [D loss: 0.582788, acc.: 95.31%] [G loss: 1.435633]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "268 [D loss: 1.122107, acc.: 31.25%] [G loss: 1.958098]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "269 [D loss: 4.505048, acc.: 0.00%] [G loss: 1.333312]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "270 [D loss: 3.495470, acc.: 0.00%] [G loss: 0.237320]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "271 [D loss: 1.439833, acc.: 50.00%] [G loss: 1.238717]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "272 [D loss: 1.157197, acc.: 0.00%] [G loss: 1.047095]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "273 [D loss: 0.912289, acc.: 0.00%] [G loss: 0.953830]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "274 [D loss: 0.899238, acc.: 0.00%] [G loss: 0.835131]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "275 [D loss: 0.854497, acc.: 0.00%] [G loss: 0.807809]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "276 [D loss: 0.829128, acc.: 0.00%] [G loss: 0.813663]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "277 [D loss: 0.820470, acc.: 0.00%] [G loss: 0.813791]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "278 [D loss: 0.813852, acc.: 0.00%] [G loss: 0.807245]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "279 [D loss: 0.809097, acc.: 0.00%] [G loss: 0.799326]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "280 [D loss: 0.802234, acc.: 0.00%] [G loss: 0.793876]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "281 [D loss: 0.796980, acc.: 0.00%] [G loss: 0.789995]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "282 [D loss: 0.793085, acc.: 0.00%] [G loss: 0.786511]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "283 [D loss: 0.788317, acc.: 0.00%] [G loss: 0.783725]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "284 [D loss: 0.785858, acc.: 0.00%] [G loss: 0.780725]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "285 [D loss: 0.781897, acc.: 0.00%] [G loss: 0.778309]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "286 [D loss: 0.779609, acc.: 0.00%] [G loss: 0.775931]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "287 [D loss: 0.776192, acc.: 0.00%] [G loss: 0.774061]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "288 [D loss: 0.773974, acc.: 0.00%] [G loss: 0.772237]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "289 [D loss: 0.771523, acc.: 0.00%] [G loss: 0.770538]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "290 [D loss: 0.769610, acc.: 0.00%] [G loss: 0.768959]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "291 [D loss: 0.767466, acc.: 0.00%] [G loss: 0.767547]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "292 [D loss: 0.765570, acc.: 0.00%] [G loss: 0.766226]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "293 [D loss: 0.763669, acc.: 0.00%] [G loss: 0.765013]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "294 [D loss: 0.762146, acc.: 0.00%] [G loss: 0.763810]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "295 [D loss: 0.760419, acc.: 0.00%] [G loss: 0.762788]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "296 [D loss: 0.759101, acc.: 0.00%] [G loss: 0.761769]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "297 [D loss: 0.757838, acc.: 0.00%] [G loss: 0.760703]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "298 [D loss: 0.756089, acc.: 0.00%] [G loss: 0.759928]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "299 [D loss: 0.755065, acc.: 0.00%] [G loss: 0.759094]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "300 [D loss: 0.753652, acc.: 0.00%] [G loss: 0.758374]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "301 [D loss: 0.752286, acc.: 0.00%] [G loss: 0.757733]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "302 [D loss: 0.751315, acc.: 0.00%] [G loss: 0.757038]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "303 [D loss: 0.750138, acc.: 0.00%] [G loss: 0.756392]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "304 [D loss: 0.749873, acc.: 0.00%] [G loss: 0.755428]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "305 [D loss: 0.748429, acc.: 0.00%] [G loss: 0.754755]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "306 [D loss: 0.747719, acc.: 0.00%] [G loss: 0.754073]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "307 [D loss: 0.746347, acc.: 0.00%] [G loss: 0.753678]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "308 [D loss: 0.745719, acc.: 0.00%] [G loss: 0.753197]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "309 [D loss: 0.744263, acc.: 0.00%] [G loss: 0.752939]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "310 [D loss: 0.744063, acc.: 0.00%] [G loss: 0.752395]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "311 [D loss: 0.742847, acc.: 0.00%] [G loss: 0.752007]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "312 [D loss: 0.741781, acc.: 0.00%] [G loss: 0.751748]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "313 [D loss: 0.741127, acc.: 0.00%] [G loss: 0.751431]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "314 [D loss: 0.740686, acc.: 0.00%] [G loss: 0.750959]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "315 [D loss: 0.740241, acc.: 0.00%] [G loss: 0.750384]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "316 [D loss: 0.740381, acc.: 0.00%] [G loss: 0.749560]\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "317 [D loss: 0.737913, acc.: 0.00%] [G loss: 0.749557]\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "318 [D loss: 0.736751, acc.: 0.00%] [G loss: 0.749808]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "319 [D loss: 0.736944, acc.: 0.00%] [G loss: 0.749602]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "320 [D loss: 0.736880, acc.: 0.00%] [G loss: 0.749058]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "321 [D loss: 0.736824, acc.: 0.00%] [G loss: 0.748343]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "322 [D loss: 0.735132, acc.: 0.00%] [G loss: 0.748132]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "323 [D loss: 0.733376, acc.: 0.00%] [G loss: 0.748429]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "324 [D loss: 0.733936, acc.: 0.00%] [G loss: 0.748255]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "325 [D loss: 0.733469, acc.: 0.00%] [G loss: 0.747931]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "326 [D loss: 0.730832, acc.: 0.00%] [G loss: 0.748348]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "327 [D loss: 0.731941, acc.: 0.00%] [G loss: 0.748148]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "328 [D loss: 0.731550, acc.: 0.00%] [G loss: 0.747760]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "329 [D loss: 0.731440, acc.: 0.00%] [G loss: 0.747259]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "330 [D loss: 0.731257, acc.: 0.00%] [G loss: 0.746734]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "331 [D loss: 0.729468, acc.: 0.00%] [G loss: 0.746777]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "332 [D loss: 0.730058, acc.: 0.00%] [G loss: 0.746508]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "333 [D loss: 0.729618, acc.: 0.00%] [G loss: 0.746198]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "334 [D loss: 0.728017, acc.: 0.00%] [G loss: 0.746305]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "335 [D loss: 0.727820, acc.: 0.00%] [G loss: 0.746333]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "336 [D loss: 0.726975, acc.: 0.00%] [G loss: 0.746418]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "337 [D loss: 0.725694, acc.: 0.00%] [G loss: 0.746716]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "338 [D loss: 0.727373, acc.: 0.00%] [G loss: 0.746230]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "339 [D loss: 0.726521, acc.: 0.00%] [G loss: 0.745843]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "340 [D loss: 0.724186, acc.: 0.00%] [G loss: 0.746165]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "341 [D loss: 0.725926, acc.: 0.00%] [G loss: 0.745815]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "342 [D loss: 0.723880, acc.: 0.00%] [G loss: 0.745930]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "343 [D loss: 0.723378, acc.: 0.00%] [G loss: 0.746108]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "344 [D loss: 0.726096, acc.: 0.00%] [G loss: 0.745233]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "345 [D loss: 0.723142, acc.: 0.00%] [G loss: 0.745177]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "346 [D loss: 0.723567, acc.: 0.00%] [G loss: 0.745039]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "347 [D loss: 0.723732, acc.: 0.00%] [G loss: 0.744737]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "348 [D loss: 0.722285, acc.: 0.00%] [G loss: 0.744796]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "349 [D loss: 0.721860, acc.: 0.00%] [G loss: 0.744909]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "350 [D loss: 0.719458, acc.: 0.00%] [G loss: 0.745595]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "351 [D loss: 0.720789, acc.: 0.00%] [G loss: 0.745670]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "352 [D loss: 0.720071, acc.: 0.00%] [G loss: 0.745675]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "353 [D loss: 0.717518, acc.: 0.00%] [G loss: 0.746310]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "354 [D loss: 0.719378, acc.: 0.00%] [G loss: 0.746212]\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "355 [D loss: 0.718271, acc.: 0.00%] [G loss: 0.746199]\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "356 [D loss: 0.718744, acc.: 0.00%] [G loss: 0.745963]\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "357 [D loss: 0.717695, acc.: 0.00%] [G loss: 0.745948]\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "358 [D loss: 0.717505, acc.: 0.00%] [G loss: 0.745930]\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "359 [D loss: 0.716147, acc.: 0.00%] [G loss: 0.746201]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "360 [D loss: 0.718234, acc.: 0.00%] [G loss: 0.745755]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "361 [D loss: 0.718936, acc.: 1.56%] [G loss: 0.744967]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "362 [D loss: 0.715216, acc.: 3.12%] [G loss: 0.745322]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "363 [D loss: 0.718801, acc.: 3.12%] [G loss: 0.744695]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "364 [D loss: 0.716585, acc.: 4.69%] [G loss: 0.744563]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "365 [D loss: 0.717184, acc.: 4.69%] [G loss: 0.744318]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "366 [D loss: 0.714110, acc.: 7.81%] [G loss: 0.744908]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "367 [D loss: 0.714042, acc.: 0.00%] [G loss: 0.745441]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "368 [D loss: 0.714493, acc.: 4.69%] [G loss: 0.745593]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "369 [D loss: 0.713672, acc.: 6.25%] [G loss: 0.745768]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "370 [D loss: 0.714091, acc.: 7.81%] [G loss: 0.745704]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "371 [D loss: 0.715108, acc.: 6.25%] [G loss: 0.745219]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "372 [D loss: 0.713459, acc.: 4.69%] [G loss: 0.745170]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "373 [D loss: 0.713775, acc.: 6.25%] [G loss: 0.745064]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "374 [D loss: 0.712710, acc.: 7.81%] [G loss: 0.745205]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "375 [D loss: 0.712015, acc.: 6.25%] [G loss: 0.745467]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "376 [D loss: 0.711279, acc.: 6.25%] [G loss: 0.745811]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "377 [D loss: 0.713352, acc.: 4.69%] [G loss: 0.745435]\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "378 [D loss: 0.713497, acc.: 6.25%] [G loss: 0.744895]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "379 [D loss: 0.712013, acc.: 4.69%] [G loss: 0.744798]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "380 [D loss: 0.707805, acc.: 21.88%] [G loss: 0.745925]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "381 [D loss: 0.709068, acc.: 15.62%] [G loss: 0.746584]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "382 [D loss: 0.709062, acc.: 7.81%] [G loss: 0.746894]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "383 [D loss: 0.710264, acc.: 14.06%] [G loss: 0.746654]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "384 [D loss: 0.711660, acc.: 9.38%] [G loss: 0.745916]\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "385 [D loss: 0.708625, acc.: 15.62%] [G loss: 0.746032]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "386 [D loss: 0.706381, acc.: 17.19%] [G loss: 0.746843]\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "387 [D loss: 0.708525, acc.: 14.06%] [G loss: 0.746953]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "388 [D loss: 0.710812, acc.: 7.81%] [G loss: 0.746199]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "389 [D loss: 0.706803, acc.: 17.19%] [G loss: 0.746480]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "390 [D loss: 0.705666, acc.: 18.75%] [G loss: 0.747169]\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "391 [D loss: 0.703362, acc.: 18.75%] [G loss: 0.748298]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "392 [D loss: 0.710168, acc.: 10.94%] [G loss: 0.747373]\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "393 [D loss: 0.704969, acc.: 20.31%] [G loss: 0.747556]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "394 [D loss: 0.705671, acc.: 14.06%] [G loss: 0.747693]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "395 [D loss: 0.705410, acc.: 15.62%] [G loss: 0.747796]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "396 [D loss: 0.706845, acc.: 14.06%] [G loss: 0.747429]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "397 [D loss: 0.703647, acc.: 20.31%] [G loss: 0.747855]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "398 [D loss: 0.703707, acc.: 20.31%] [G loss: 0.748281]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "399 [D loss: 0.702006, acc.: 20.31%] [G loss: 0.748992]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "400 [D loss: 0.702980, acc.: 18.75%] [G loss: 0.749283]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "401 [D loss: 0.702634, acc.: 18.75%] [G loss: 0.749428]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "402 [D loss: 0.703094, acc.: 21.88%] [G loss: 0.749360]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "403 [D loss: 0.702774, acc.: 20.31%] [G loss: 0.749292]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "404 [D loss: 0.699360, acc.: 26.56%] [G loss: 0.750071]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "405 [D loss: 0.703828, acc.: 18.75%] [G loss: 0.749642]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "406 [D loss: 0.698957, acc.: 20.31%] [G loss: 0.750264]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "407 [D loss: 0.702683, acc.: 18.75%] [G loss: 0.749966]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "408 [D loss: 0.704885, acc.: 14.06%] [G loss: 0.748940]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "409 [D loss: 0.704343, acc.: 18.75%] [G loss: 0.748111]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "410 [D loss: 0.703082, acc.: 17.19%] [G loss: 0.747808]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "411 [D loss: 0.698264, acc.: 21.88%] [G loss: 0.748873]\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "412 [D loss: 0.699582, acc.: 21.88%] [G loss: 0.749630]\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "413 [D loss: 0.702424, acc.: 17.19%] [G loss: 0.749352]\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "414 [D loss: 0.704129, acc.: 15.62%] [G loss: 0.748449]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "415 [D loss: 0.701149, acc.: 23.44%] [G loss: 0.748350]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "416 [D loss: 0.698110, acc.: 25.00%] [G loss: 0.749184]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "417 [D loss: 0.701111, acc.: 21.88%] [G loss: 0.749226]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "418 [D loss: 0.700348, acc.: 20.31%] [G loss: 0.749227]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "419 [D loss: 0.700215, acc.: 23.44%] [G loss: 0.749219]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "420 [D loss: 0.695901, acc.: 26.56%] [G loss: 0.750264]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "421 [D loss: 0.694697, acc.: 26.56%] [G loss: 0.751550]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "422 [D loss: 0.697165, acc.: 25.00%] [G loss: 0.751911]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "423 [D loss: 0.702107, acc.: 12.50%] [G loss: 0.750699]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "424 [D loss: 0.700464, acc.: 20.31%] [G loss: 0.749813]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "425 [D loss: 0.697248, acc.: 25.00%] [G loss: 0.749984]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "426 [D loss: 0.696653, acc.: 29.69%] [G loss: 0.750466]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "427 [D loss: 0.697132, acc.: 20.31%] [G loss: 0.750739]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "428 [D loss: 0.698824, acc.: 21.88%] [G loss: 0.750460]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "429 [D loss: 0.694137, acc.: 29.69%] [G loss: 0.751264]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "430 [D loss: 0.701791, acc.: 21.88%] [G loss: 0.750193]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "431 [D loss: 0.701711, acc.: 14.06%] [G loss: 0.748924]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "432 [D loss: 0.695385, acc.: 23.44%] [G loss: 0.749473]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "433 [D loss: 0.699933, acc.: 18.75%] [G loss: 0.749131]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "434 [D loss: 0.695226, acc.: 26.56%] [G loss: 0.749792]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "435 [D loss: 0.696802, acc.: 21.88%] [G loss: 0.750131]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "436 [D loss: 0.701788, acc.: 12.50%] [G loss: 0.749048]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "437 [D loss: 0.693335, acc.: 28.12%] [G loss: 0.749940]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "438 [D loss: 0.686589, acc.: 37.50%] [G loss: 0.752701]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "439 [D loss: 0.694439, acc.: 23.44%] [G loss: 0.753278]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "440 [D loss: 0.692553, acc.: 78.12%] [G loss: 0.753633]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "441 [D loss: 0.698809, acc.: 17.19%] [G loss: 0.752303]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "442 [D loss: 0.692568, acc.: 28.12%] [G loss: 0.752419]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "443 [D loss: 0.688823, acc.: 79.69%] [G loss: 0.753712]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "444 [D loss: 0.685206, acc.: 85.94%] [G loss: 0.755810]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "445 [D loss: 0.689988, acc.: 76.56%] [G loss: 0.756403]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "446 [D loss: 0.688221, acc.: 79.69%] [G loss: 0.756934]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "447 [D loss: 0.698957, acc.: 64.06%] [G loss: 0.754688]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "448 [D loss: 0.690703, acc.: 78.12%] [G loss: 0.754314]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "449 [D loss: 0.689026, acc.: 76.56%] [G loss: 0.754831]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "450 [D loss: 0.692169, acc.: 76.56%] [G loss: 0.754653]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "451 [D loss: 0.690059, acc.: 81.25%] [G loss: 0.754858]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "452 [D loss: 0.691652, acc.: 76.56%] [G loss: 0.754675]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "453 [D loss: 0.690582, acc.: 76.56%] [G loss: 0.754706]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "454 [D loss: 0.686186, acc.: 79.69%] [G loss: 0.755802]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "455 [D loss: 0.688111, acc.: 82.81%] [G loss: 0.756405]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "456 [D loss: 0.691118, acc.: 76.56%] [G loss: 0.756004]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "457 [D loss: 0.690311, acc.: 79.69%] [G loss: 0.755645]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "458 [D loss: 0.691440, acc.: 76.56%] [G loss: 0.755089]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "459 [D loss: 0.688987, acc.: 75.00%] [G loss: 0.755186]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "460 [D loss: 0.690857, acc.: 75.00%] [G loss: 0.754928]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "461 [D loss: 0.689736, acc.: 76.56%] [G loss: 0.754899]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "462 [D loss: 0.691358, acc.: 73.44%] [G loss: 0.754522]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "463 [D loss: 0.688908, acc.: 73.44%] [G loss: 0.754733]\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "464 [D loss: 0.688374, acc.: 79.69%] [G loss: 0.755141]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "465 [D loss: 0.682437, acc.: 82.81%] [G loss: 0.756899]\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "466 [D loss: 0.690791, acc.: 73.44%] [G loss: 0.756516]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "467 [D loss: 0.689053, acc.: 75.00%] [G loss: 0.756133]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "468 [D loss: 0.687596, acc.: 78.12%] [G loss: 0.756168]\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "469 [D loss: 0.684217, acc.: 79.69%] [G loss: 0.757070]\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "470 [D loss: 0.687310, acc.: 75.00%] [G loss: 0.757219]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "471 [D loss: 0.683902, acc.: 79.69%] [G loss: 0.757953]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "472 [D loss: 0.682400, acc.: 81.25%] [G loss: 0.758986]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "473 [D loss: 0.688710, acc.: 78.12%] [G loss: 0.758330]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "474 [D loss: 0.686355, acc.: 78.12%] [G loss: 0.758000]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "475 [D loss: 0.687909, acc.: 73.44%] [G loss: 0.757439]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "476 [D loss: 0.682430, acc.: 79.69%] [G loss: 0.758225]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "477 [D loss: 0.680654, acc.: 81.25%] [G loss: 0.759534]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "478 [D loss: 0.681169, acc.: 85.94%] [G loss: 0.760489]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "479 [D loss: 0.679230, acc.: 84.38%] [G loss: 0.761616]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "480 [D loss: 0.689812, acc.: 68.75%] [G loss: 0.760038]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "481 [D loss: 0.679472, acc.: 81.25%] [G loss: 0.760592]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "482 [D loss: 0.690699, acc.: 73.44%] [G loss: 0.758867]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "483 [D loss: 0.681740, acc.: 79.69%] [G loss: 0.759075]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "484 [D loss: 0.679187, acc.: 81.25%] [G loss: 0.760297]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "485 [D loss: 0.683812, acc.: 78.12%] [G loss: 0.760382]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "486 [D loss: 0.679327, acc.: 81.25%] [G loss: 0.761199]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "487 [D loss: 0.682346, acc.: 78.12%] [G loss: 0.761261]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "488 [D loss: 0.682679, acc.: 78.12%] [G loss: 0.761085]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "489 [D loss: 0.670009, acc.: 87.50%] [G loss: 0.763851]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "490 [D loss: 0.689858, acc.: 71.88%] [G loss: 0.762008]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "491 [D loss: 0.680324, acc.: 81.25%] [G loss: 0.761704]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "492 [D loss: 0.686350, acc.: 76.56%] [G loss: 0.760387]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "493 [D loss: 0.673715, acc.: 85.94%] [G loss: 0.762083]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "494 [D loss: 0.677569, acc.: 87.50%] [G loss: 0.763158]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "495 [D loss: 0.676040, acc.: 81.25%] [G loss: 0.764220]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "496 [D loss: 0.679697, acc.: 78.12%] [G loss: 0.764167]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "497 [D loss: 0.674924, acc.: 85.94%] [G loss: 0.764977]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "498 [D loss: 0.680862, acc.: 75.00%] [G loss: 0.764429]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "499 [D loss: 0.674933, acc.: 84.38%] [G loss: 0.765060]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.6261 - accuracy: 1.0000\n",
            "Accuracy on generated music samples: 1.0\n",
            "1/1 [==============================] - 0s 66ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, LSTM, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pretty_midi\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the generator with additional parameters for tempo, chord progression, and instrument\n",
        "def build_generator(latent_dim, num_notes, num_instruments):\n",
        "    # Input layers for noise, tempo, chord progression, and instrument\n",
        "    noise = Input(shape=(latent_dim,))\n",
        "    tempo = Input(shape=(1,))\n",
        "    chord_progression = Input(shape=(num_chords,))\n",
        "    instrument = Input(shape=(num_instruments,))\n",
        "\n",
        "    # Concatenate the inputs\n",
        "    x = concatenate([noise, tempo, chord_progression, instrument])\n",
        "\n",
        "    # Generator network architecture\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    output_notes = Dense(num_notes, activation='sigmoid')(x)\n",
        "    return Model([noise, tempo, chord_progression, instrument], output_notes)\n",
        "\n",
        "# Define the discriminator (unchanged)\n",
        "def build_discriminator(num_notes):\n",
        "    music = Input(shape=(num_notes,))\n",
        "    x = Reshape((num_notes, 1))(music)\n",
        "    x = LSTM(512)(x)\n",
        "    validity = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(music, validity)\n",
        "\n",
        "# Define the dimensions and parameters\n",
        "latent_dim = 100\n",
        "num_notes = 128  # Number of notes in a music piece\n",
        "num_chords = 4   # Number of chords in the chord progression\n",
        "num_instruments = 128  # Total number of instruments (adjust as needed)\n",
        "epochs = 500      # Reduced number of epochs\n",
        "batch_size = 32   # Reduced batch size for faster training\n",
        "\n",
        "# Placeholder for training data (replace this with your actual training data)\n",
        "X = np.random.rand(1000, num_notes)\n",
        "X_train, X_test_validate = train_test_split(X, test_size=0.2, random_state=42)\n",
        "X_test, X_validate = train_test_split(X_test_validate, test_size=0.5, random_state=42)\n",
        "\n",
        "# Build the generator and discriminator\n",
        "generator = build_generator(latent_dim, num_notes, num_instruments)\n",
        "discriminator = build_discriminator(num_notes)\n",
        "\n",
        "# Compile the discriminator\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# The generator takes noise, tempo, chord progression, and instrument as input and generates music\n",
        "noise = Input(shape=(latent_dim,))\n",
        "tempo = Input(shape=(1,))\n",
        "chord_progression = Input(shape=(num_chords,))\n",
        "instrument = Input(shape=(num_instruments,))\n",
        "generated_music = generator([noise, tempo, chord_progression, instrument])\n",
        "\n",
        "# For the combined model, only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated music as input and determines validity\n",
        "validity = discriminator(generated_music)\n",
        "\n",
        "# The combined model (stacked generator and discriminator)\n",
        "combined = Model([noise, tempo, chord_progression, instrument], validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of music\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_music = X_train[idx]\n",
        "\n",
        "    # Generate a batch of new music\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    # Adjust the instrument to violin (41 is the program number for Violin)\n",
        "    instrument_data = np.ones((batch_size, num_notes)) * 41\n",
        "\n",
        "    # Adjust the chord progression to CFCG\n",
        "    chord_progression_data = np.array([[0, 5, 7, 10]] * batch_size)\n",
        "\n",
        "    # Adjust the tempo to 120 BPM\n",
        "    tempo_data = np.ones((batch_size, 1)) * 120\n",
        "\n",
        "    generated_music = generator.predict([noise, tempo_data, chord_progression_data, instrument_data])\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_music, np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(generated_music, np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    g_loss = combined.train_on_batch([noise, tempo_data, chord_progression_data, instrument_data], np.ones((batch_size, 1)))\n",
        "\n",
        "    # Print progress\n",
        "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "# Testing the model\n",
        "# Select a random batch of music from the test set\n",
        "idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
        "real_music_test = X_test[idx]\n",
        "\n",
        "# Generate a batch of new music\n",
        "noise_test = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "instrument_data_test = np.ones((batch_size, num_notes)) * 41  # Adjust the instrument to violin\n",
        "chord_progression_data_test = np.array([[0, 5, 7, 10]] * batch_size)  # Adjust the chord progression to CFCG\n",
        "tempo_data_test = np.ones((batch_size, 1)) * 120  # Adjust the tempo to 120 BPM\n",
        "\n",
        "generated_music_test = generator.predict([noise_test, tempo_data_test, chord_progression_data_test, instrument_data_test])\n",
        "\n",
        "# Evaluate the generated music with the discriminator\n",
        "accuracy = discriminator.evaluate(generated_music_test, np.zeros((batch_size, 1)))[1]\n",
        "print(\"Accuracy on generated music samples:\", accuracy)\n",
        "\n",
        "# Generate some music samples (adjust num_samples as needed)\n",
        "def generate_music(generator, num_samples=1):\n",
        "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "    tempo_data = np.ones((num_samples, 1)) * 120  # Tempo set to 120 BPM for all samples\n",
        "    chord_progression_data = np.array([[0, 5, 7, 10]] * num_samples)  # Chord progression set to CFCG for all samples\n",
        "    instrument_data = np.ones((num_samples, num_instruments)) * 41  # Instrument set to Violin (41) for all samples\n",
        "    generated_music = generator.predict([noise, tempo_data, chord_progression_data, instrument_data])\n",
        "    return generated_music\n",
        "\n",
        "# Example usage\n",
        "generated_samples = generate_music(generator, num_samples=5)\n",
        "\n",
        "# Save the generated samples as MIDI files\n",
        "for i, sample in enumerate(generated_samples):\n",
        "    midi_data = pretty_midi.PrettyMIDI()\n",
        "    violin_program = pretty_midi.instrument_name_to_program('Violin')\n",
        "    violin = pretty_midi.Instrument(program=violin_program)\n",
        "    for time, note_prob in enumerate(sample):\n",
        "        if note_prob > 0.5:  # Play the note if the probability is above a threshold\n",
        "            pitch = np.random.randint(0, 128)  # Random pitch between 0 and 127\n",
        "            velocity = np.random.randint(0, 128)  # Random velocity between 0 and 127\n",
        "            note_start = time * 0.5  # Each time step is 0.5 seconds\n",
        "            note_end = note_start + 0.5\n",
        "            midi_note = pretty_midi.Note(\n",
        "                velocity=velocity, pitch=pitch, start=note_start, end=note_end)\n",
        "            violin.notes.append(midi_note)\n",
        "    midi_data.instruments.append(violin)\n",
        "    midi_data.write(f'generated_music_{i}.mid')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJvaddRLH6Kc"
      },
      "source": [
        "CGAN Train test with piano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzrarj80GLvX",
        "outputId": "1f499f7b-e071-45a3-f620-221b26c6f7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 96ms/step\n",
            "0 [D loss: 0.697426, acc.: 50.00%] [G loss: 0.629991]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1 [D loss: 0.682362, acc.: 50.00%] [G loss: 0.624395]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "2 [D loss: 0.667508, acc.: 50.00%] [G loss: 0.632577]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3 [D loss: 0.651804, acc.: 50.00%] [G loss: 0.628267]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "4 [D loss: 0.630241, acc.: 50.00%] [G loss: 0.628940]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "5 [D loss: 0.584093, acc.: 50.00%] [G loss: 0.642988]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "6 [D loss: 0.403724, acc.: 64.06%] [G loss: 1.450142]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "7 [D loss: 0.079470, acc.: 100.00%] [G loss: 2.736530]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "8 [D loss: 0.021271, acc.: 100.00%] [G loss: 3.789123]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "9 [D loss: 0.008327, acc.: 100.00%] [G loss: 4.584059]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "10 [D loss: 0.004176, acc.: 100.00%] [G loss: 5.190630]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "11 [D loss: 0.002416, acc.: 100.00%] [G loss: 5.706038]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "12 [D loss: 0.001681, acc.: 100.00%] [G loss: 6.074677]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "13 [D loss: 0.001264, acc.: 100.00%] [G loss: 6.392405]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "14 [D loss: 0.001003, acc.: 100.00%] [G loss: 6.666539]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "15 [D loss: 0.000829, acc.: 100.00%] [G loss: 6.916207]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "16 [D loss: 0.000714, acc.: 100.00%] [G loss: 7.127005]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "17 [D loss: 0.000636, acc.: 100.00%] [G loss: 7.282274]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "18 [D loss: 0.000571, acc.: 100.00%] [G loss: 7.471062]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "19 [D loss: 0.000525, acc.: 100.00%] [G loss: 7.598032]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "20 [D loss: 0.000487, acc.: 100.00%] [G loss: 7.729887]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "21 [D loss: 0.000457, acc.: 100.00%] [G loss: 7.849643]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "22 [D loss: 0.000431, acc.: 100.00%] [G loss: 7.937965]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "23 [D loss: 0.000409, acc.: 100.00%] [G loss: 8.027529]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "24 [D loss: 0.000388, acc.: 100.00%] [G loss: 8.101053]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "25 [D loss: 0.000372, acc.: 100.00%] [G loss: 8.189766]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "26 [D loss: 0.000355, acc.: 100.00%] [G loss: 8.253714]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "27 [D loss: 0.000339, acc.: 100.00%] [G loss: 8.314642]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "28 [D loss: 0.000325, acc.: 100.00%] [G loss: 8.372248]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "29 [D loss: 0.000315, acc.: 100.00%] [G loss: 8.444481]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "30 [D loss: 0.000301, acc.: 100.00%] [G loss: 8.473584]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "31 [D loss: 0.000290, acc.: 100.00%] [G loss: 8.529491]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "32 [D loss: 0.000280, acc.: 100.00%] [G loss: 8.583817]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "33 [D loss: 0.000269, acc.: 100.00%] [G loss: 8.621077]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "34 [D loss: 0.000261, acc.: 100.00%] [G loss: 8.675945]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "35 [D loss: 0.000252, acc.: 100.00%] [G loss: 8.716232]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "36 [D loss: 0.000243, acc.: 100.00%] [G loss: 8.742115]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "37 [D loss: 0.000237, acc.: 100.00%] [G loss: 8.806386]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "38 [D loss: 0.000228, acc.: 100.00%] [G loss: 8.826845]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "39 [D loss: 0.000222, acc.: 100.00%] [G loss: 8.865103]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "40 [D loss: 0.000215, acc.: 100.00%] [G loss: 8.893504]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "41 [D loss: 0.000209, acc.: 100.00%] [G loss: 8.938787]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "42 [D loss: 0.000202, acc.: 100.00%] [G loss: 8.959447]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "43 [D loss: 0.000197, acc.: 100.00%] [G loss: 9.000631]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "44 [D loss: 0.000192, acc.: 100.00%] [G loss: 9.031392]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "45 [D loss: 0.000185, acc.: 100.00%] [G loss: 9.043886]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "46 [D loss: 0.000180, acc.: 100.00%] [G loss: 9.070641]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "47 [D loss: 0.000175, acc.: 100.00%] [G loss: 9.102321]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "48 [D loss: 0.000171, acc.: 100.00%] [G loss: 9.134548]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "49 [D loss: 0.000167, acc.: 100.00%] [G loss: 9.162601]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "50 [D loss: 0.000162, acc.: 100.00%] [G loss: 9.183446]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "51 [D loss: 0.000157, acc.: 100.00%] [G loss: 9.187618]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "52 [D loss: 0.000155, acc.: 100.00%] [G loss: 9.247571]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "53 [D loss: 0.000150, acc.: 100.00%] [G loss: 9.248584]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "54 [D loss: 0.000146, acc.: 100.00%] [G loss: 9.274000]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "55 [D loss: 0.000143, acc.: 100.00%] [G loss: 9.315304]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "56 [D loss: 0.000140, acc.: 100.00%] [G loss: 9.341337]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "57 [D loss: 0.000137, acc.: 100.00%] [G loss: 9.363455]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "58 [D loss: 0.000134, acc.: 100.00%] [G loss: 9.389111]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "59 [D loss: 0.000130, acc.: 100.00%] [G loss: 9.390572]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "60 [D loss: 0.000128, acc.: 100.00%] [G loss: 9.430938]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "61 [D loss: 0.000125, acc.: 100.00%] [G loss: 9.455763]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "62 [D loss: 0.000122, acc.: 100.00%] [G loss: 9.468354]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "63 [D loss: 0.000120, acc.: 100.00%] [G loss: 9.499823]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "64 [D loss: 0.000118, acc.: 100.00%] [G loss: 9.529006]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "65 [D loss: 0.000114, acc.: 100.00%] [G loss: 9.515555]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "66 [D loss: 0.000112, acc.: 100.00%] [G loss: 9.547388]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "67 [D loss: 0.000109, acc.: 100.00%] [G loss: 9.566645]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "68 [D loss: 0.000108, acc.: 100.00%] [G loss: 9.592802]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "69 [D loss: 0.000105, acc.: 100.00%] [G loss: 9.602461]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "70 [D loss: 0.000103, acc.: 100.00%] [G loss: 9.618452]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "71 [D loss: 0.000102, acc.: 100.00%] [G loss: 9.659250]\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "72 [D loss: 0.000099, acc.: 100.00%] [G loss: 9.667341]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "73 [D loss: 0.000097, acc.: 100.00%] [G loss: 9.677103]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "74 [D loss: 0.000095, acc.: 100.00%] [G loss: 9.687466]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "75 [D loss: 0.000093, acc.: 100.00%] [G loss: 9.708985]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "76 [D loss: 0.000091, acc.: 100.00%] [G loss: 9.722754]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "77 [D loss: 0.000089, acc.: 100.00%] [G loss: 9.742103]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "78 [D loss: 0.000088, acc.: 100.00%] [G loss: 9.748465]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "79 [D loss: 0.000087, acc.: 100.00%] [G loss: 9.787012]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "80 [D loss: 0.000085, acc.: 100.00%] [G loss: 9.794605]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "81 [D loss: 0.000083, acc.: 100.00%] [G loss: 9.809793]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "82 [D loss: 0.000082, acc.: 100.00%] [G loss: 9.841109]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "83 [D loss: 0.000080, acc.: 100.00%] [G loss: 9.853024]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "84 [D loss: 0.000079, acc.: 100.00%] [G loss: 9.879567]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "85 [D loss: 0.000077, acc.: 100.00%] [G loss: 9.879435]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "86 [D loss: 0.000076, acc.: 100.00%] [G loss: 9.875587]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "87 [D loss: 0.000075, acc.: 100.00%] [G loss: 9.925688]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "88 [D loss: 0.000074, acc.: 100.00%] [G loss: 9.926603]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "89 [D loss: 0.000072, acc.: 100.00%] [G loss: 9.949780]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "90 [D loss: 0.000071, acc.: 100.00%] [G loss: 9.951321]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "91 [D loss: 0.000070, acc.: 100.00%] [G loss: 9.981816]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "92 [D loss: 0.000069, acc.: 100.00%] [G loss: 9.980400]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "93 [D loss: 0.000068, acc.: 100.00%] [G loss: 9.998289]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "94 [D loss: 0.000067, acc.: 100.00%] [G loss: 10.021149]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "95 [D loss: 0.000065, acc.: 100.00%] [G loss: 10.019964]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "96 [D loss: 0.000064, acc.: 100.00%] [G loss: 10.038818]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "97 [D loss: 0.000063, acc.: 100.00%] [G loss: 10.050152]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "98 [D loss: 0.000062, acc.: 100.00%] [G loss: 10.065642]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "99 [D loss: 0.000062, acc.: 100.00%] [G loss: 10.093582]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "100 [D loss: 0.000060, acc.: 100.00%] [G loss: 10.098873]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "101 [D loss: 0.000059, acc.: 100.00%] [G loss: 10.093937]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "102 [D loss: 0.000059, acc.: 100.00%] [G loss: 10.137197]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "103 [D loss: 0.000058, acc.: 100.00%] [G loss: 10.137718]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "104 [D loss: 0.000057, acc.: 100.00%] [G loss: 10.166938]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "105 [D loss: 0.000056, acc.: 100.00%] [G loss: 10.165221]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "106 [D loss: 0.000055, acc.: 100.00%] [G loss: 10.188049]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "107 [D loss: 0.000054, acc.: 100.00%] [G loss: 10.199363]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "108 [D loss: 0.000054, acc.: 100.00%] [G loss: 10.216714]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "109 [D loss: 0.000053, acc.: 100.00%] [G loss: 10.209340]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "110 [D loss: 0.000052, acc.: 100.00%] [G loss: 10.244693]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "111 [D loss: 0.000051, acc.: 100.00%] [G loss: 10.220192]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "112 [D loss: 0.000051, acc.: 100.00%] [G loss: 10.264843]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "113 [D loss: 0.000050, acc.: 100.00%] [G loss: 10.255580]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "114 [D loss: 0.000049, acc.: 100.00%] [G loss: 10.286133]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "115 [D loss: 0.000048, acc.: 100.00%] [G loss: 10.285386]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "116 [D loss: 0.000048, acc.: 100.00%] [G loss: 10.315397]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "117 [D loss: 0.000047, acc.: 100.00%] [G loss: 10.318487]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "118 [D loss: 0.000047, acc.: 100.00%] [G loss: 10.333450]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "119 [D loss: 0.000046, acc.: 100.00%] [G loss: 10.349291]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "120 [D loss: 0.000045, acc.: 100.00%] [G loss: 10.351036]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "121 [D loss: 0.000045, acc.: 100.00%] [G loss: 10.364914]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "122 [D loss: 0.000044, acc.: 100.00%] [G loss: 10.344763]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "123 [D loss: 0.000044, acc.: 100.00%] [G loss: 10.393394]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "124 [D loss: 0.000043, acc.: 100.00%] [G loss: 10.403212]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "125 [D loss: 0.000043, acc.: 100.00%] [G loss: 10.409290]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "126 [D loss: 0.000042, acc.: 100.00%] [G loss: 10.420693]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "127 [D loss: 0.000042, acc.: 100.00%] [G loss: 10.453794]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "128 [D loss: 0.000041, acc.: 100.00%] [G loss: 10.430843]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "129 [D loss: 0.000040, acc.: 100.00%] [G loss: 10.437875]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "130 [D loss: 0.000040, acc.: 100.00%] [G loss: 10.453233]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "131 [D loss: 0.000040, acc.: 100.00%] [G loss: 10.484133]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "132 [D loss: 0.000039, acc.: 100.00%] [G loss: 10.495665]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "133 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.486373]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "134 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.504976]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "135 [D loss: 0.000037, acc.: 100.00%] [G loss: 10.507810]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "136 [D loss: 0.000037, acc.: 100.00%] [G loss: 10.546310]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "137 [D loss: 0.000037, acc.: 100.00%] [G loss: 10.555385]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "138 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.537642]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "139 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.555252]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "140 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.584887]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "141 [D loss: 0.000035, acc.: 100.00%] [G loss: 10.559792]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "142 [D loss: 0.000035, acc.: 100.00%] [G loss: 10.586452]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "143 [D loss: 0.000034, acc.: 100.00%] [G loss: 10.600407]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "144 [D loss: 0.000034, acc.: 100.00%] [G loss: 10.611786]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "145 [D loss: 0.000033, acc.: 100.00%] [G loss: 10.607322]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "146 [D loss: 0.000033, acc.: 100.00%] [G loss: 10.623004]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "147 [D loss: 0.000033, acc.: 100.00%] [G loss: 10.636010]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "148 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.633898]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "149 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.654167]\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "150 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.670544]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "151 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.676394]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "152 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.683577]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "153 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.707787]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "154 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.726430]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "155 [D loss: 0.000030, acc.: 100.00%] [G loss: 10.732882]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "156 [D loss: 0.000030, acc.: 100.00%] [G loss: 10.719542]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "157 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.728570]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "158 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.738905]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "159 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.750883]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "160 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.763503]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "161 [D loss: 0.000028, acc.: 100.00%] [G loss: 10.770115]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "162 [D loss: 0.000028, acc.: 100.00%] [G loss: 10.777386]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "163 [D loss: 0.000028, acc.: 100.00%] [G loss: 10.787344]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "164 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.798520]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "165 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.819213]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "166 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.810182]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "167 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.853333]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "168 [D loss: 0.000026, acc.: 100.00%] [G loss: 10.840931]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "169 [D loss: 0.000026, acc.: 100.00%] [G loss: 10.851237]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "170 [D loss: 0.000026, acc.: 100.00%] [G loss: 10.877814]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "171 [D loss: 0.000026, acc.: 100.00%] [G loss: 10.870775]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "172 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.871532]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "173 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.881952]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "174 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.898739]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "175 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.885843]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "176 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.899260]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "177 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.912978]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "178 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.936827]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "179 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.921409]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "180 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.956507]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "181 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.965592]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "182 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.971463]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "183 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.949561]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "184 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.976881]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "185 [D loss: 0.000022, acc.: 100.00%] [G loss: 10.978378]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "186 [D loss: 0.000022, acc.: 100.00%] [G loss: 10.993313]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "187 [D loss: 0.000022, acc.: 100.00%] [G loss: 11.018359]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "188 [D loss: 0.000022, acc.: 100.00%] [G loss: 11.022623]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "189 [D loss: 0.000022, acc.: 100.00%] [G loss: 11.004708]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "190 [D loss: 0.000022, acc.: 100.00%] [G loss: 11.025061]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "191 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.029660]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "192 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.016298]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "193 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.062993]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "194 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.066326]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "195 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.071159]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "196 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.081856]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "197 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.108424]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "198 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.108338]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "199 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.097863]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "200 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.114769]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "201 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.138191]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "202 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.133259]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "203 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.134361]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "204 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.127721]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "205 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.141426]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "206 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.187521]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "207 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.152887]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "208 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.183368]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "209 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.190250]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "210 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.192812]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "211 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.198591]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "212 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.197471]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "213 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.207364]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "214 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.234282]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "215 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.213190]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "216 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.203098]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "217 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.231115]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "218 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.248609]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "219 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.276474]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "220 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.246963]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "221 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.254811]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "222 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.255014]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "223 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.277498]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "224 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.287239]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "225 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.311034]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "226 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.288453]\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "227 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.322627]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "228 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.307352]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "229 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.296879]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "230 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.333891]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "231 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.313677]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "232 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.317839]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "233 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.336927]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "234 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.366337]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "235 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.346120]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "236 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.353041]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "237 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.365755]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "238 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.377341]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "239 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.380390]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "240 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.381941]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "241 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.392270]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "242 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.392210]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "243 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.409540]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "244 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.415096]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "245 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.415466]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "246 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.413794]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "247 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.433031]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "248 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.416516]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "249 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.412130]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "250 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.423028]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "251 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.454288]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "252 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.447130]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "253 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.466396]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "254 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.465978]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "255 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.462938]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "256 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.494841]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "257 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.481098]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "258 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.498304]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "259 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.490522]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "260 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.513423]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "261 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.503917]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "262 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.505554]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "263 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.530203]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "264 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.531689]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "265 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.530912]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "266 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.538687]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "267 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.522428]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "268 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.534840]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "269 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.544703]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "270 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.545271]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "271 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.600780]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "272 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.570423]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "273 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.591536]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "274 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.567664]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "275 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.581200]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "276 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.618781]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "277 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.614672]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "278 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.585422]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "279 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.612677]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "280 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.616407]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "281 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.610458]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "282 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.623024]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "283 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.613373]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "284 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.635578]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "285 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.638545]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "286 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.641034]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "287 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.637468]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "288 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.652317]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "289 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.653679]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "290 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.683590]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "291 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.665949]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "292 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.663298]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "293 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.672606]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "294 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.691786]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "295 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.669737]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "296 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.678360]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "297 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.710207]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "298 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.695136]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "299 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.710428]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "300 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.728895]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "301 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.711834]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "302 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.727618]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "303 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.721931]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "304 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.730755]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "305 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.724042]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "306 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.751732]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "307 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.715100]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "308 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.743605]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "309 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.734364]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "310 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.760000]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "311 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.762650]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "312 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.752873]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "313 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.792377]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "314 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.791362]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "315 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.786089]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "316 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.783642]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "317 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.802610]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "318 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.810860]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "319 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.802919]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "320 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.803261]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "321 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.799703]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "322 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.808643]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "323 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.825644]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "324 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.803528]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "325 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.835159]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "326 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.814835]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "327 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.832145]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "328 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.844030]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "329 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.822974]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "330 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.842999]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "331 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.850404]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "332 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.861090]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "333 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.859532]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "334 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.891051]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "335 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.881201]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "336 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.879557]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "337 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.893030]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "338 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.876547]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "339 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.880164]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "340 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.900803]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "341 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.895931]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "342 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.919075]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "343 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.894107]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "344 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.915359]\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "345 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.880247]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "346 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.915365]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "347 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.910864]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "348 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.926180]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "349 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.926642]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "350 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.916430]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "351 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.960941]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "352 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.950926]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "353 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.959717]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "354 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.946793]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "355 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.958331]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "356 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.941532]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "357 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.956969]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "358 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.953862]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "359 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.976992]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "360 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.977768]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "361 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.950342]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "362 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.974092]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "363 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.974370]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "364 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.976879]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "365 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.981482]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "366 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.980288]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "367 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.009424]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "368 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.007399]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "369 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.028266]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "370 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.021420]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "371 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.002308]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "372 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.990719]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "373 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.030753]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "374 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.039037]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "375 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.042490]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "376 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.004765]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "377 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.025713]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "378 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.048000]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "379 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.061614]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "380 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.054007]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "381 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.058088]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "382 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.056022]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "383 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.041543]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "384 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.070910]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "385 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.041471]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "386 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.069189]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "387 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.090205]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "388 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.068399]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "389 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.085978]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "390 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.091446]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "391 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.074121]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "392 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.096610]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "393 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.108620]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "394 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.098345]\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "395 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.124254]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "396 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.109640]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "397 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.118093]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "398 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.113123]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "399 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.107735]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "400 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.115242]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "401 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.126524]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "402 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.135534]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "403 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.132923]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "404 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.137871]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "405 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.144316]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "406 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.131857]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "407 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.153296]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "408 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.157022]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "409 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.141315]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "410 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.159969]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "411 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.167929]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "412 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.166763]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "413 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.172092]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "414 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.209673]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "415 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.197108]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "416 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.182417]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "417 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.181597]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "418 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.187577]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "419 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.221907]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "420 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.215281]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "421 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.186351]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "422 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.200380]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "423 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.206262]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "424 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.214825]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "425 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.228548]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "426 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.228354]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "427 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.227518]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "428 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.213750]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "429 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.223717]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "430 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.240026]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "431 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.246115]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "432 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.248495]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "433 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.231971]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "434 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.240357]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "435 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.259241]\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "436 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.252950]\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "437 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.238611]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "438 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.272011]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "439 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.272426]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "440 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.277758]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "441 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.283954]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "442 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.277724]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "443 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.259068]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "444 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.261894]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "445 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.266333]\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "446 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.263686]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "447 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.275400]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "448 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.270351]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "449 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.296368]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "450 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.301448]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "451 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.283953]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "452 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.318071]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "453 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.308506]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "454 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.302868]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "455 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.316681]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "456 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.322909]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "457 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.314642]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "458 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.313667]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "459 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.326587]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "460 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.319857]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "461 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.316790]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "462 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.314508]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "463 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.326681]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "464 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.333310]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "465 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.348465]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "466 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.336279]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "467 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.353174]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "468 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.349300]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "469 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.374289]\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "470 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.362856]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "471 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.370479]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "472 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.356095]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "473 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.387917]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "474 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.367641]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "475 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.387262]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "476 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.390681]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "477 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.398671]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "478 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.383366]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "479 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.401163]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "480 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.373833]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "481 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.397791]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "482 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.405998]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "483 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.413876]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "484 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.390558]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "485 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.411287]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "486 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.407789]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "487 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.394686]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "488 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.415951]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "489 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.413551]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "490 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.436805]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "491 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.425570]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "492 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.421778]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "493 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.417703]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "494 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.425073]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "495 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.445718]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "496 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.447668]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "497 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.453352]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "498 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.446626]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "499 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.457315]\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x78ad2fdfbe20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 460ms/step - loss: 3.9361e-06 - accuracy: 1.0000\n",
            "Accuracy on generated music samples: 1.0\n",
            "1/1 [==============================] - 0s 63ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, LSTM, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pretty_midi\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the generator\n",
        "def build_generator(latent_dim, num_notes, num_genres):\n",
        "    notes_input = Input(shape=(latent_dim,))\n",
        "    genre_input = Input(shape=(num_genres,))\n",
        "    concatenated = Concatenate()([notes_input, genre_input])\n",
        "    x = Dense(256, activation='relu')(concatenated)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    output_notes = Dense(num_notes, activation='softmax')(x)\n",
        "    return Model([notes_input, genre_input], output_notes)\n",
        "\n",
        "# Define the discriminator\n",
        "def build_discriminator(num_notes, num_genres):\n",
        "    music_input = Input(shape=(num_notes,))\n",
        "    genre_input = Input(shape=(num_genres,))\n",
        "    x = Reshape((num_notes, 1))(music_input)\n",
        "    x = LSTM(512)(x)\n",
        "    x = Concatenate()([x, genre_input])\n",
        "    validity = Dense(1, activation='sigmoid')(x)\n",
        "    return Model([music_input, genre_input], validity)\n",
        "\n",
        "# Define the dimensions and parameters\n",
        "latent_dim = 100\n",
        "num_notes = 128  # Number of notes in a music piece\n",
        "num_genres = 5  # Number of genres\n",
        "epochs = 500\n",
        "batch_size = 32\n",
        "output_length = 15  # Duration of output MIDI in seconds\n",
        "\n",
        "# Placeholder for training data (replace this with your actual training data)\n",
        "X = np.random.rand(1000, num_notes)\n",
        "y = np.random.randint(0, 2, (1000, num_genres))\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)\n",
        "\n",
        "# Build and compile the discriminator\n",
        "discriminator = build_discriminator(num_notes, num_genres)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "generator = build_generator(latent_dim, num_notes, num_genres)\n",
        "\n",
        "# The generator takes random noise and genre as input and generates music\n",
        "noise_input = Input(shape=(latent_dim,))\n",
        "genre_input = Input(shape=(num_genres,))\n",
        "generated_music = generator([noise_input, genre_input])\n",
        "\n",
        "# For the combined model, only train the generator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# The discriminator takes generated music and genre as input and determines validity\n",
        "validity = discriminator([generated_music, genre_input])\n",
        "\n",
        "# The combined model (stacked generator and discriminator)\n",
        "combined = Model([noise_input, genre_input], validity)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Select a random batch of music and genre\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_music = X_train[idx]\n",
        "    real_genre = y_train[idx]\n",
        "\n",
        "    # Generate a batch of new music\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    generated_music = generator.predict([noise, real_genre])\n",
        "\n",
        "    # Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch([real_music, real_genre], np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch([generated_music, real_genre], np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train the generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    g_loss = combined.train_on_batch([noise, real_genre], np.ones((batch_size, 1)))\n",
        "\n",
        "    # Print progress\n",
        "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "# Testing the model\n",
        "# Select a random batch of music and genre from the test set\n",
        "idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
        "real_music_test = X_test[idx]\n",
        "real_genre_test = y_test[idx]\n",
        "\n",
        "# Generate a batch of new music\n",
        "noise_test = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "generated_music_test = generator.predict([noise_test, real_genre_test])\n",
        "\n",
        "# Evaluate the generated music with the discriminator\n",
        "accuracy = discriminator.evaluate([generated_music_test, real_genre_test], np.zeros((batch_size, 1)))[1]\n",
        "print(\"Accuracy on generated music samples:\", accuracy)\n",
        "\n",
        "# Generate some music samples (adjust num_samples as needed)\n",
        "num_samples = 5\n",
        "generated_samples = generator.predict([np.random.normal(0, 1, (num_samples, latent_dim)), y_test[:num_samples]])\n",
        "\n",
        "# Save the generated samples as MIDI files\n",
        "for i, sample in enumerate(generated_samples):\n",
        "    midi_data = pretty_midi.PrettyMIDI()\n",
        "    instrument_program = pretty_midi.instrument_name_to_program('Violin')\n",
        "    instrument_track = pretty_midi.Instrument(program=instrument_program)\n",
        "    for time, note_prob in enumerate(sample):\n",
        "        if note_prob > 0.3:  # Play the note if the probability is above a threshold\n",
        "            pitch = np.random.randint(0, 128)  # Random pitch between 0 and 127\n",
        "            velocity = np.random.randint(0, 128)  # Random velocity between 0 and 127\n",
        "            note_start = time * (60 / 120)  # Each time step is 60 / tempo seconds\n",
        "            note_end = note_start + (60 / 120)  # Length of each note\n",
        "            midi_note = pretty_midi.Note(\n",
        "                velocity=velocity, pitch=pitch, start=note_start, end=note_end)\n",
        "            instrument_track.notes.append(midi_note)\n",
        "    # Add C major chord progression\n",
        "    for j, chord_note in enumerate([60, 64, 67]):  # C, E, G\n",
        "        chord_start = j * (60 / 120)  # Each chord lasts for 1 beat\n",
        "        chord_end = chord_start + (60 / 120)  # Length of each chord\n",
        "        midi_note = pretty_midi.Note(\n",
        "            velocity=0, pitch=chord_note, start=chord_start, end=chord_end)\n",
        "        instrument_track.notes.append(midi_note)\n",
        "    midi_data.instruments.append(instrument_track)\n",
        "    midi_data.write(f'generated_music_{i}.mid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gqm8C9SIugK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
